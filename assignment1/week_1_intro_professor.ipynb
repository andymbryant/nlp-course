{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Week 1: Introduction and Overview\n",
    "This notebook accompanies the week 1 lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'spacy-transformers'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "import transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Natural Language Tool Kit vs SpaCy\n",
    "For this course, we will mainly be using [SpaCy]().  There are a [number of other]() NLP libraries and probably one of the best known is the [Natural Language Tool Kit (NLTK)]().  SpaCy and NLTK both are very powerful, but here I'll show a couple of reasons why I prefer SpaCy.\n",
    "\n",
    "Note: You will not be able to run these on your own without installing NLTK.  Since it's not used in the rest of the course, I'm not configuring it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "[(We, <class 'spacy.tokens.token.Token'>), (are, <class 'spacy.tokens.token.Token'>), (doing, <class 'spacy.tokens.token.Token'>), (NLP, <class 'spacy.tokens.token.Token'>), (., <class 'spacy.tokens.token.Token'>)]\n"
     ]
    }
   ],
   "source": [
    "# spacy\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "text = 'We are doing NLP.'\n",
    "doc = en(text)\n",
    "print(type(doc))\n",
    "print([(x, type(x)) for x in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[('We', <class 'str'>), ('are', <class 'str'>), ('doing', <class 'str'>), ('NLP', <class 'str'>), ('.', <class 'str'>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andybryant/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "doc = word_tokenize(text)\n",
    "print(type(doc))\n",
    "print([(x, type(x)) for x in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.8 µs ± 26.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "21.9 µs ± 1.19 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# time comparison\n",
    "%timeit en(text)\n",
    "%timeit nltk.tokenize.casual_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5 µs ± 37.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "4.1 µs ± 16.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing: lower case and removing non-alpha\n",
    "text = 'We are doing NLP.'\n",
    "# spacy\n",
    "%timeit  [x.lower_ for x in en(text) if x.is_alpha]\n",
    "# nltk (one possible way)\n",
    "%timeit nltk.RegexpTokenizer(r'\\w+').tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You can see both libraries are very powerful.  But SpaCy's syntax is a bit simpler and it's generally a bit faster.  Have you tried NLTK or other libraries? Let's discuss.\n",
    "\n",
    "### SpaCy's language models\n",
    "We'll cover this more extensively in the slides.  But here's some illustration of what's going on under the hood with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at the different languages supported by spacy\n",
    "#from spacy.lang import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.tokenizer.Tokenizer object at 0x153e107a0>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# what's in a language model\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "print(en.tokenizer)\n",
    "print(en.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# read in English model with tagging/entity pipeline components\n",
    "# you will need to run the line below beforehand\n",
    "#!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are doing NLP\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "{'__delattr__', 'remove_extension', 'merge', '__class__', 'lang_', '__dir__', '__unicode__', 'doc', '__getitem__', 'sents', '__eq__', 'from_bytes', '_realloc', '__hash__', 'tensor', 'is_sentenced', 'is_tagged', 'extend_tensor', 'print_tree', '__gt__', 'cats', 'is_parsed', 'user_span_hooks', '__subclasshook__', 'to_array', '_vector', '__doc__', '__pyx_vtable__', '__le__', '__setattr__', 'vector_norm', '__iter__', '__format__', 'user_token_hooks', '__lt__', 'is_nered', 'user_hooks', '__reduce_ex__', '__sizeof__', '_', 'noun_chunks_iterator', 'retokenize', '_vector_norm', '__bytes__', 'set_extension', 'to_utf8_array', 'mem', 'has_vector', 'to_json', 'user_data', 'vocab', 'noun_chunks', '__repr__', '__init_subclass__', 'from_disk', 'vector', '__str__', 'get_lca_matrix', 'sentiment', 'from_array', 'get_extension', 'ents', 'has_extension', 'count_by', 'text_with_ws', '__len__', 'text', '_bulk_merge', 'to_disk', '__setstate__', '__getattribute__', '__reduce__', 'similarity', 'lang', '__ne__', '__ge__', '__init__', '__new__', 'char_span', '_py_tokens', 'to_bytes'}\n",
      "We are\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "{'n_rights', 'as_doc', 'start', 'ent_id_', 'sent', 'lower_', 'root', 'conjuncts', 'string', 'lefts', 'orth_', 'end', 'label_', 'end_char', '_recalculate_indices', 'n_lefts', 'kb_id', 'label', 'start_char', '_fix_dep_copy', 'lemma_', 'ent_id', 'kb_id_', 'subtree', 'upper_', 'rights'}\n",
      "We\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_', 'doc', 'ent_id', 'ent_id_', 'ent_iob', 'ent_iob_', 'ent_kb_id', 'ent_kb_id_', 'ent_type', 'ent_type_', 'get_extension', 'has_extension', 'has_vector', 'head', 'i', 'idx', 'is_alpha', 'is_ancestor', 'is_ascii', 'is_bracket', 'is_currency', 'is_digit', 'is_left_punct', 'is_lower', 'is_oov', 'is_punct', 'is_quote', 'is_right_punct', 'is_sent_end', 'is_sent_start', 'is_space', 'is_stop', 'is_title', 'is_upper', 'lang', 'lang_', 'left_edge', 'lefts', 'lemma', 'lemma_', 'lex_id', 'like_email', 'like_num', 'like_url', 'lower', 'lower_', 'morph', 'n_lefts', 'n_rights', 'nbor', 'norm', 'norm_', 'orth', 'orth_', 'pos', 'pos_', 'prefix', 'prefix_', 'prob', 'rank', 'remove_extension', 'right_edge', 'rights', 'sent', 'sent_start', 'sentiment', 'set_extension', 'shape', 'shape_', 'similarity', 'string', 'subtree', 'suffix', 'suffix_', 'tag', 'tag_', 'tensor', 'text', 'text_with_ws', 'vector', 'vector_norm', 'vocab', 'whitespace_']\n"
     ]
    }
   ],
   "source": [
    "# turning text into a spacy document\n",
    "doc = en('We are doing NLP')\n",
    "print(doc)\n",
    "print(type(doc))\n",
    "doc_attrs = set(dir(doc))\n",
    "print(doc_attrs)\n",
    "# spans: subsets of doc\n",
    "print(doc[:2])\n",
    "print(type(doc[:2]))\n",
    "span_attrs = set(dir(doc[:2]))\n",
    "print(span_attrs - doc_attrs)\n",
    "# tokens: individual units of doc\n",
    "print(doc[0])\n",
    "print(type(doc[0]))\n",
    "print(dir(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16064069575701507746\n",
      "we\n"
     ]
    }
   ],
   "source": [
    "# internally, spacy represents tokens as hash values\n",
    "print(doc[0].lower)\n",
    "en.vocab[doc[0].lower].text\n",
    "# but you probably won't need that often\n",
    "print(doc[0].lower_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "This section shows some of the considerations to make when tokenizing your data.\n",
    "\n",
    "Token = \"Useful semantic unit\"\n",
    "\n",
    "But what does that mean? This section will detail some considerations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Jieba not installed. Either set the default to False with `from spacy.lang.zh import ChineseDefaults; ChineseDefaults.use_jieba = False`, or install it with `pip install jieba` or from https://github.com/fxsjy/jieba",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/spacy/lang/zh/__init__.py\u001b[0m in \u001b[0;36mtry_jieba_import\u001b[0;34m(use_jieba)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jieba'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-30da49145117>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzh\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChinese\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mzh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChinese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mzh_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'我们正在做NLP。'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tokenize in Chinese:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzh_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab, make_doc, max_length, meta, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmake_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mfactory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mmake_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/spacy/lang/zh/__init__.py\u001b[0m in \u001b[0;36mcreate_tokenizer\u001b[0;34m(cls, nlp, config)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mChineseTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/spacy/lang/zh/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cls, nlp, config)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_pkuseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"require_pkuseg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjieba_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_jieba_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_jieba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         self.pkuseg_seg = try_pkuseg_import(\n\u001b[1;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_pkuseg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/spacy/lang/zh/__init__.py\u001b[0m in \u001b[0;36mtry_jieba_import\u001b[0;34m(use_jieba)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;34m\"https://github.com/fxsjy/jieba\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             )\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Jieba not installed. Either set the default to False with `from spacy.lang.zh import ChineseDefaults; ChineseDefaults.use_jieba = False`, or install it with `pip install jieba` or from https://github.com/fxsjy/jieba"
     ]
    }
   ],
   "source": [
    "# importing different languages in spacy\n",
    "# blank English model\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "# blank Chinese model\n",
    "# to run, will need to install jieba tokenizer (optional)\n",
    "#!pip install jieba\n",
    "from spacy.lang.zh import Chinese\n",
    "\n",
    "zh = spacy.lang.zh.Chinese()\n",
    "zh_text = '我们正在做NLP。'\n",
    "print('Tokenize in Chinese:', [x.text for x in zh(zh_text)])\n",
    "print('Tokenize in English:', [x.text for x in en(zh_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f19c584cb739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# interesting tidbit: the base Chinese model uses the jieba tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mzh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# any other languages that might require a custom tokenization strategy?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'zh' is not defined"
     ]
    }
   ],
   "source": [
    "# interesting tidbit: the base Chinese model uses the jieba tokenizer\n",
    "zh.tokenizer\n",
    "# any other languages that might require a custom tokenization strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# lowercasing\n",
    "text = 'We are doing NLP.'\n",
    "print('Base python: ', text.lower())\n",
    "print('SpaCy:', [x.lower_ for x in en(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are doing NLP\n",
      "['We', 'are', 'doing', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "# handling non-alpha\n",
    "text = 'We are doing NLP.'\n",
    "# base python\n",
    "strip_punct = '[^A-Za-z0-9 ]'\n",
    "print(re.sub(strip_punct, '', text))\n",
    "# spacy\n",
    "print([x.text for x in en(text) if x.is_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just removing punctuation: Were doing NLP\n",
      "Removing non-alpha ['We', 'doing', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "# but what about contractions?\n",
    "text = \"We're doing NLP.\"\n",
    "# base python\n",
    "strip_punct = '[^A-Za-z0-9 ]'\n",
    "print('Just removing punctuation:', re.sub(strip_punct, '', text))\n",
    "# spacy\n",
    "print('Removing non-alpha', [x.text for x in en(text) if x.is_alpha])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the is_alpha flag is False for any tokens that have non-alpha characters.  We'll look into a better way for dealing with contractions later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a tokenizer\n",
    "In this exercise, you will make a function that uses spaCy's base English model to tokenize a dataset according to specific parameters.  The functions will take a list of documents and output a list of tokens.  In this case we're interested in outputting strings, rather than spaCy tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "en = English()\n",
    "\n",
    "def tokenize_base(docs, model=en):\n",
    "    # tokenizer that just parses using spaCy's base model\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        tokenized_docs.append([t.text for t in parsed])\n",
    "    return(tokenized_docs)\n",
    "\n",
    "def tokenize_lower_alpha(docs, model=en):\n",
    "    # tokenizer that lowercases and removes any non-alpha character\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        tokenized_docs.append([t.lower_ for t in parsed if t.is_alpha])\n",
    "    return(tokenized_docs)\n",
    "\n",
    "def tokenize_lower_alpha_url(docs, model=en):\n",
    "    # tokenizer that lowercases, removes any non-alpha character and removes urls\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        tokenized_docs.append([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])\n",
    "    return(tokenized_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming\n",
    "Though word tense can sometimes carry with it a lot of useful information, a lot of time it might be useful to reduce words to their common root.  For example, the word \"be\" has various forms like \"are\", \"is\", \"been\".  We might not want our vocabulary to contain all these forms and rather count them all as instances of \"be\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am taking an NLP course.\n",
      "['-PRON-', 'be', 'take', 'an', 'NLP', 'course', '.']\n"
     ]
    }
   ],
   "source": [
    "# read in English model with tagging/entity pipeline components\n",
    "# you will need to run the line below beforehand\n",
    "#!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'I am taking an NLP course.'\n",
    "print(text)\n",
    "print([x.lemma_ for x in nlp(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are recording this class with Zoom.\n",
      "['-PRON-', 'be', 'record', 'this', 'class', 'with', 'Zoom', '.']\n"
     ]
    }
   ],
   "source": [
    "# guess how lemmatization might change this\n",
    "text = 'We are recording this class with Zoom.'\n",
    "guess = ''\n",
    "print(text)\n",
    "print([x.lemma_ for x in nlp(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "Dealing with stop words involves making some pretty impactful decisions with your data.  Refer to the slides for details.  Here, we just remove stop words based on [spaCy's default set](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In June 2020, I took a course at Harvard Extension School in Cambridge.\n",
      "['June', '2020', ',', 'took', 'course', 'Harvard', 'Extension', 'School', 'Cambridge', '.']\n"
     ]
    }
   ],
   "source": [
    "en = English()\n",
    "text = 'In June 2020, I took a course at Harvard Extension School in Cambridge.'\n",
    "print(text)\n",
    "print([x.text for x in en(text) if not x.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-standard tokens (e.g. named-entities)\n",
    "In text, some some n-grams should not be treated as a concatenation of unigrams.  For example, New York City is fundamentally different from the individual words \"new\", \"york\" and \"city\".\n",
    "\n",
    "Here we attempt to deal with some of these non-standard tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out these courses: https://www.summer.harvard.edu/\n",
      "Check out these courses: \n",
      "[Check, out, these, courses, :]\n",
      "[Check, out, these, courses, :, '-URL-']\n"
     ]
    }
   ],
   "source": [
    "# urls\n",
    "# base python\n",
    "# regex from textacy: https://github.com/chartbeat-labs/textacy\n",
    "SHORT_URL_REGEX = re.compile(\n",
    "    r\"(?:^|(?<![\\w/.]))\"\n",
    "    # optional scheme\n",
    "    r\"(?:(?:https?://)?)\"\n",
    "    # domain\n",
    "    r\"(?:\\w-?)*?\\w+(?:\\.[a-z]{2,12}){1,3}\"\n",
    "    r\"/+\",\n",
    "    flags=re.IGNORECASE)\n",
    "text = 'Check out these courses: https://www.summer.harvard.edu/'\n",
    "print(text)\n",
    "print(SHORT_URL_REGEX.sub('', text))\n",
    "# spacy\n",
    "print([x for x in en(text) if not x.like_url])\n",
    "# spacy - replace with a standard token\n",
    "print(['-URL-' if x.like_url else x for x in en(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, am, taking, an, NLP, course, at, Harvard, starting, July, 19th, ,, 2020]\n",
      "NLP <class 'spacy.tokens.span.Span'> ORG Companies, agencies, institutions, etc.\n",
      "Harvard <class 'spacy.tokens.span.Span'> ORG Companies, agencies, institutions, etc.\n",
      "July 19th, 2020 <class 'spacy.tokens.span.Span'> DATE Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "# named-entities\n",
    "# read in English model with tagging/entity pipeline components\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'I am taking an NLP course at Harvard starting July 19th, 2020'\n",
    "parsed = nlp(text)\n",
    "# look at the individual tokens\n",
    "tokens = [t for t in parsed]\n",
    "print(tokens)\n",
    "# look at the identified named-entities and their types\n",
    "for e in parsed.ents:\n",
    "    print(e, type(e), e.label_, spacy.explain(e.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: A comprehensive tokenization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_full(docs, model=nlp, \n",
    "                  entities=False, \n",
    "                  stop_words=False, \n",
    "                  lowercase=True, \n",
    "                  alpha_only=True, \n",
    "                  lemma=True):\n",
    "    \"\"\"Full tokenizer with flags for processing steps\n",
    "    entities: If False, replaces with entity type\n",
    "    stop_words: If False, removes stop words\n",
    "    lowercase: If True, lowercases all tokens\n",
    "    alpha_only: If True, removes all non-alpha characters\n",
    "    lemma: If True, lemmatizes words\n",
    "    \"\"\"\n",
    "    tokenized_docs = []\n",
    "    for d in docs:\n",
    "        parsed = model(d)\n",
    "        # token collector\n",
    "        tokens = []\n",
    "        # index pointer\n",
    "        i = 0\n",
    "        # entity collector\n",
    "        ent = ''\n",
    "        for t in parsed:\n",
    "            # only need this if we're replacing entities\n",
    "            if not entities:\n",
    "                # replace URLs\n",
    "                if t.like_url:\n",
    "                    tokens.append('URL')\n",
    "                    continue\n",
    "                # if there's entities collected and current token is non-entity\n",
    "                if (t.ent_iob_=='O')&(ent!=''):\n",
    "                    tokens.append(ent)\n",
    "                    ent = ''\n",
    "                    continue\n",
    "                elif t.ent_iob_!='O':\n",
    "                    ent = t.ent_type_\n",
    "                    continue\n",
    "            # only include stop words if stop words==True\n",
    "            if (t.is_stop)&(not stop_words):\n",
    "                continue\n",
    "            # only include non-alpha is alpha_only==False\n",
    "            if (not t.is_alpha)&(alpha_only):\n",
    "                continue\n",
    "            if lemma:\n",
    "                t = t.lemma_\n",
    "            else:\n",
    "                t = t.text\n",
    "            if lowercase:\n",
    "                t.lower()\n",
    "            tokens.append(t)\n",
    "        tokenized_docs.append(tokens)\n",
    "    return(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-PRON-', 'be', 'take', 'a', 'course', 'at', 'Harvard', '.'],\n",
       " ['-PRON-', 'be', 'learn', 'about', 'Natural', 'Language', 'Processing', '.'],\n",
       " ['-PRON-',\n",
       "  'be',\n",
       "  'study',\n",
       "  'tokenization',\n",
       "  ',',\n",
       "  'vectorization',\n",
       "  'and',\n",
       "  'modelling',\n",
       "  '.'],\n",
       " ['check',\n",
       "  'out',\n",
       "  'the',\n",
       "  'course',\n",
       "  'on',\n",
       "  'Github',\n",
       "  ':',\n",
       "  'https://github.com/bpben/nlp_lesson']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_full(text_data, stop_words=True, alpha_only=False, entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counts\n",
    "A very basic way to use a sanitized list of tokens is to do a word count.  This unlocks a lot of insights right off and is an important step in exploratory data analysis in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "\n",
    "def simple_tokenizer(doc, model=en):\n",
    "    # a simple tokenizer for individual documents (different from above)\n",
    "    tokenized_docs = []\n",
    "    parsed = model(doc)\n",
    "    return([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]\n",
    "tokenized = [simple_tokenizer(doc) for doc in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of counts: [Counter({'i': 1, 'taking': 1, 'a': 1, 'course': 1, 'at': 1, 'harvard': 1}), Counter({'i': 1, 'learning': 1, 'about': 1, 'natural': 1, 'language': 1, 'processing': 1}), Counter({'we': 1, 'are': 1, 'studying': 1, 'tokenization': 1, 'vectorization': 1, 'and': 1, 'modelling': 1}), Counter({'check': 1, 'out': 1, 'the': 1, 'course': 1, 'on': 1, 'github': 1})]\n",
      "[Counter({'i': 1, 'taking': 1, 'a': 1, 'course': 1, 'at': 1, 'harvard': 1}), Counter({'i': 1, 'learning': 1, 'about': 1, 'natural': 1, 'language': 1, 'processing': 1}), Counter({'we': 1, 'are': 1, 'studying': 1, 'tokenization': 1, 'vectorization': 1, 'and': 1, 'modelling': 1}), Counter({'check': 1, 'out': 1, 'the': 1, 'course': 1, 'on': 1, 'github': 1})]\n",
      "\n",
      "Combined count: Counter({'i': 2, 'course': 2, 'taking': 1, 'a': 1, 'at': 1, 'harvard': 1, 'learning': 1, 'about': 1, 'natural': 1, 'language': 1, 'processing': 1, 'we': 1, 'are': 1, 'studying': 1, 'tokenization': 1, 'vectorization': 1, 'and': 1, 'modelling': 1, 'check': 1, 'out': 1, 'the': 1, 'on': 1, 'github': 1})\n"
     ]
    }
   ],
   "source": [
    "# base python: create an make use of a Counter object\n",
    "counts = [Counter(d) for d in tokenized]\n",
    "print('List of counts:', counts)\n",
    "# sum together all counts\n",
    "all_counts = Counter()\n",
    "for d in tokenized:\n",
    "    all_counts += Counter(d)\n",
    "print(counts)\n",
    "print('\\nCombined count:', all_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn's countvectorizer\n",
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'about': 1,\n",
       " 'and': 1,\n",
       " 'are': 1,\n",
       " 'at': 1,\n",
       " 'check': 1,\n",
       " 'course': 2,\n",
       " 'github': 1,\n",
       " 'harvard': 1,\n",
       " 'i': 2,\n",
       " 'language': 1,\n",
       " 'learning': 1,\n",
       " 'modelling': 1,\n",
       " 'natural': 1,\n",
       " 'on': 1,\n",
       " 'out': 1,\n",
       " 'processing': 1,\n",
       " 'studying': 1,\n",
       " 'taking': 1,\n",
       " 'the': 1,\n",
       " 'tokenization': 1,\n",
       " 'vectorization': 1,\n",
       " 'we': 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use our custom tokenizer\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "# outputs sparse array, want to use a normal numpy array\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "dict(zip(cv.get_feature_names(), v.sum(axis=0)))\n",
    "# result is the same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>at</th>\n",
       "      <th>bpben</th>\n",
       "      <th>check</th>\n",
       "      <th>com</th>\n",
       "      <th>course</th>\n",
       "      <th>github</th>\n",
       "      <th>harvard</th>\n",
       "      <th>...</th>\n",
       "      <th>nlp_lessons</th>\n",
       "      <th>on</th>\n",
       "      <th>out</th>\n",
       "      <th>processing</th>\n",
       "      <th>studying</th>\n",
       "      <th>taking</th>\n",
       "      <th>the</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>vectorization</th>\n",
       "      <th>we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   about  and  are  at  bpben  check  com  course  github  harvard  ...  \\\n",
       "0      0    0    0   1      0      0    0       1       0        1  ...   \n",
       "1      1    0    0   0      0      0    0       0       0        0  ...   \n",
       "2      0    1    1   0      0      0    0       0       0        0  ...   \n",
       "3      0    0    0   0      1      1    1       1       2        0  ...   \n",
       "\n",
       "   nlp_lessons  on  out  processing  studying  taking  the  tokenization  \\\n",
       "0            0   0    0           0         0       1    0             0   \n",
       "1            0   0    0           1         0       0    0             0   \n",
       "2            0   0    0           0         1       0    0             1   \n",
       "3            1   1    1           0         0       0    1             0   \n",
       "\n",
       "   vectorization  we  \n",
       "0              0   0  \n",
       "1              0   0  \n",
       "2              1   1  \n",
       "3              0   0  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# other neat uses of CountVectorizer\n",
    "# turning it into a dataframe for easier manipulation\n",
    "cv = CountVectorizer()\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "pd.DataFrame(v, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>'</th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>/</th>\n",
       "      <th>:</th>\n",
       "      <th>_</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>...</th>\n",
       "      <th>o</th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      '  ,  .  /  :  _  a  b  c  ...  o  p  r  s  t  u  v  w  y  z\n",
       "0  5  1  0  1  0  0  0  5  0  1  ...  1  0  3  1  2  1  1  0  0  0\n",
       "1  5  1  0  1  0  0  0  6  1  1  ...  2  1  3  2  2  3  0  0  0  0\n",
       "2  6  0  1  1  0  0  0  4  0  1  ...  5  0  2  1  5  1  1  1  1  2\n",
       "3  6  0  0  1  4  2  1  0  4  4  ...  5  3  1  5  6  4  0  0  0  0\n",
       "\n",
       "[4 rows x 29 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# character-level\n",
    "cv = CountVectorizer(analyzer='char')\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "pd.DataFrame(v, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>about natural</th>\n",
       "      <th>and</th>\n",
       "      <th>and modelling</th>\n",
       "      <th>are</th>\n",
       "      <th>are studying</th>\n",
       "      <th>at</th>\n",
       "      <th>at harvard</th>\n",
       "      <th>bpben</th>\n",
       "      <th>bpben nlp_lessons</th>\n",
       "      <th>...</th>\n",
       "      <th>taking</th>\n",
       "      <th>taking course</th>\n",
       "      <th>the</th>\n",
       "      <th>the course</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>tokenization vectorization</th>\n",
       "      <th>vectorization</th>\n",
       "      <th>vectorization and</th>\n",
       "      <th>we</th>\n",
       "      <th>we are</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   about  about natural  and  and modelling  are  are studying  at  \\\n",
       "0      0              0    0              0    0             0   1   \n",
       "1      1              1    0              0    0             0   0   \n",
       "2      0              0    1              1    1             1   0   \n",
       "3      0              0    0              0    0             0   0   \n",
       "\n",
       "   at harvard  bpben  bpben nlp_lessons  ...  taking  taking course  the  \\\n",
       "0           1      0                  0  ...       1              1    0   \n",
       "1           0      0                  0  ...       0              0    0   \n",
       "2           0      0                  0  ...       0              0    0   \n",
       "3           0      1                  1  ...       0              0    1   \n",
       "\n",
       "   the course  tokenization  tokenization vectorization  vectorization  \\\n",
       "0           0             0                           0              0   \n",
       "1           0             0                           0              0   \n",
       "2           0             1                           1              1   \n",
       "3           1             0                           0              0   \n",
       "\n",
       "   vectorization and  we  we are  \n",
       "0                  0   0       0  \n",
       "1                  0   0       0  \n",
       "2                  1   1       1  \n",
       "3                  0   0       0  \n",
       "\n",
       "[4 rows x 48 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-grams\n",
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "pd.DataFrame(v, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>natural</th>\n",
       "      <th>language</th>\n",
       "      <th>processing</th>\n",
       "      <th>harvard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   natural  language  processing  harvard\n",
       "0        0         0           0        1\n",
       "1        1         1           1        0\n",
       "2        0         0           0        0\n",
       "3        0         0           0        0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-specified vocabulary\n",
    "cv = CountVectorizer(vocabulary=['natural', 'language', 'processing', 'harvard'])\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "pd.DataFrame(v, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Sentiment analysis with word counts\n",
    "Imagine you are a hot dog restaurant owner and you want to analyze a corpus of reviews from diners to see whether people generally think your hot dogs are \"good\" or \"bad\".  Specifically, you're going to count up the number of times the word \"good\" and word \"bad\" appears.  Depending on how you process the text, you will arrive at different conclusions.  Try a couple ways to see what I mean.\n",
    "\n",
    "You might also want to think about whether all the reviews are relevant.  Those sorts of choices may also affect your results.  Is there an automatic way you can remove non-relevant reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['These hot dogs are really good.',\n",
    "          'These hot dogs are really bad.',\n",
    "          'Good hot dogs!',\n",
    "          'The hot dogs pair well with a Good Humor bar.',\n",
    "          \"I didn't eat anything, I felt bad.\",\n",
    "          \"I had a good time!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "en = English()\n",
    "def simple_tokenizer(doc, model=en):\n",
    "    # a simple tokenizer for individual documents (different from above)\n",
    "    tokenized_docs = []\n",
    "    parsed = model(doc)\n",
    "    return([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good count: 4\n",
      "Bad count: 2\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "v = cv.fit_transform(reviews).toarray()\n",
    "count_df = pd.DataFrame(v, columns=cv.get_feature_names())\n",
    "print('Good count:', count_df['good'].sum())\n",
    "print('Bad count:', count_df['bad'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>anything</th>\n",
       "      <th>are</th>\n",
       "      <th>bad</th>\n",
       "      <th>bar</th>\n",
       "      <th>did</th>\n",
       "      <th>dogs</th>\n",
       "      <th>eat</th>\n",
       "      <th>felt</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>hot</th>\n",
       "      <th>humor</th>\n",
       "      <th>i</th>\n",
       "      <th>pair</th>\n",
       "      <th>really</th>\n",
       "      <th>the</th>\n",
       "      <th>these</th>\n",
       "      <th>time</th>\n",
       "      <th>well</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  anything  are  bad  bar  did  dogs  eat  felt  good  ...  hot  humor  i  \\\n",
       "0  0         0    1    0    0    0     1    0     0     1  ...    1      0  0   \n",
       "1  0         0    1    1    0    0     1    0     0     0  ...    1      0  0   \n",
       "2  0         0    0    0    0    0     1    0     0     1  ...    1      0  0   \n",
       "3  1         0    0    0    1    0     1    0     0     1  ...    1      1  0   \n",
       "4  0         1    0    1    0    1     0    1     1     0  ...    0      0  2   \n",
       "5  1         0    0    0    0    0     0    0     0     1  ...    0      0  1   \n",
       "\n",
       "   pair  really  the  these  time  well  with  \n",
       "0     0       1    0      1     0     0     0  \n",
       "1     0       1    0      1     0     0     0  \n",
       "2     0       0    0      0     0     0     0  \n",
       "3     1       0    1      0     0     1     1  \n",
       "4     0       0    0      0     0     0     0  \n",
       "5     0       0    0      0     1     0     0  \n",
       "\n",
       "[6 rows x 21 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good count: 2\n",
      "Bad count: 2\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(lowercase=False,\n",
    "                     # to make it comparable, need to ensure single-character tokens don't get removed,\n",
    "                     token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "v = cv.fit_transform(reviews).toarray()\n",
    "count_df = pd.DataFrame(v, columns=cv.get_feature_names())\n",
    "print('Good count:', count_df['good'].sum())\n",
    "print('Bad count:', count_df['bad'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Good</th>\n",
       "      <th>Humor</th>\n",
       "      <th>I</th>\n",
       "      <th>The</th>\n",
       "      <th>These</th>\n",
       "      <th>a</th>\n",
       "      <th>anything</th>\n",
       "      <th>are</th>\n",
       "      <th>bad</th>\n",
       "      <th>bar</th>\n",
       "      <th>...</th>\n",
       "      <th>felt</th>\n",
       "      <th>good</th>\n",
       "      <th>had</th>\n",
       "      <th>hot</th>\n",
       "      <th>pair</th>\n",
       "      <th>really</th>\n",
       "      <th>t</th>\n",
       "      <th>time</th>\n",
       "      <th>well</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Good  Humor  I  The  These  a  anything  are  bad  bar  ...  felt  good  \\\n",
       "0     0      0  0    0      1  0         0    1    0    0  ...     0     1   \n",
       "1     0      0  0    0      1  0         0    1    1    0  ...     0     0   \n",
       "2     1      0  0    0      0  0         0    0    0    0  ...     0     0   \n",
       "3     1      1  0    1      0  1         0    0    0    1  ...     0     0   \n",
       "4     0      0  2    0      0  0         1    0    1    0  ...     1     0   \n",
       "5     0      0  1    0      0  1         0    0    0    0  ...     0     1   \n",
       "\n",
       "   had  hot  pair  really  t  time  well  with  \n",
       "0    0    1     0       1  0     0     0     0  \n",
       "1    0    1     0       1  0     0     0     0  \n",
       "2    0    1     0       0  0     0     0     0  \n",
       "3    0    1     1       0  0     0     1     1  \n",
       "4    0    0     0       0  1     0     0     0  \n",
       "5    1    0     0       0  0     1     0     0  \n",
       "\n",
       "[6 rows x 23 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple statistical test: Log-likelihood ratio\n",
    "Above we just compared the count of the word good to the count of the word bad.  But we can actually test the significance of this difference using a test of log-likelihood ratio.  Refer to the slides for a bit more information, but here's a calculation based on the above example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log, mean\n",
    "def log_likelihood(analysis, reference, word):\n",
    "    # count of word in source\n",
    "    a = analysis[word].sum()\n",
    "    # count of word in reference\n",
    "    b = reference[word].sum()\n",
    "    # count of all words in source\n",
    "    c = analysis.sum().sum()\n",
    "    # count of all words in reference\n",
    "    d = reference.sum().sum()\n",
    "    print('counts analysis:', a)\n",
    "    print('counts reference:', b)\n",
    "    e1 = c*(a+b)/(c+d)\n",
    "    e2 = d*(a+b)/(c+d)\n",
    "    g = 2*((a*log(a/e1)) + (b*log(b/e2)))\n",
    "    print('G2: ', g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Good  Humor  I  The  These  a  anything  are  bad  bar  ...  felt  good  \\\n",
      "0     0      0  0    0      1  0         0    1    0    0  ...     0     1   \n",
      "1     0      0  0    0      1  0         0    1    1    0  ...     0     0   \n",
      "2     1      0  0    0      0  0         0    0    0    0  ...     0     0   \n",
      "3     1      1  0    1      0  1         0    0    0    1  ...     0     0   \n",
      "\n",
      "   had  hot  pair  really  t  time  well  with  \n",
      "0    0    1     0       1  0     0     0     0  \n",
      "1    0    1     0       1  0     0     0     0  \n",
      "2    0    1     0       0  0     0     0     0  \n",
      "3    0    1     1       0  0     0     1     1  \n",
      "\n",
      "[4 rows x 23 columns]\n",
      "(2, 23)\n",
      "counts analysis: 1\n",
      "counts reference: 1\n",
      "G2:  0.21010555200628656\n"
     ]
    }
   ],
   "source": [
    "# analysis: hot dog texts\n",
    "analysis = count_df[(count_df['hot'] + count_df['dogs'])==2]\n",
    "print(analysis)\n",
    "# print(analysis.shape)\n",
    "# reference: non hot dog texts\n",
    "reference = count_df[(count_df['hot'] + count_df['dogs'])!=2]\n",
    "print(reference.shape)\n",
    "\n",
    "log_likelihood(analysis, reference, 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts analysis: 13\n",
      "counts reference: 1\n",
      "G2:  3.317957773087898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# this is pretty non-significant\n",
    "# G2=3.84 means 5% of getting this difference by chance, this is closer to 30%\n",
    "# if we added a few to analysis\n",
    "analysis['good'] = analysis['good']+3\n",
    "log_likelihood(analysis, reference, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to advanced models\n",
    "In this section, we'll be setting up some of the requirements for the more advanced techniques we will cover later in the course.  Particularly, we'll be working with:\n",
    "\n",
    "- [huggingface's transformers library](https://github.com/huggingface/transformers)\n",
    "- [spaCy-transformers (based on the above)](https://github.com/explosion/spacy-transformers)\n",
    "\n",
    "These require some additional downloads.  For these examples you'll need:\n",
    "\n",
    "[BERT uncased large model](https://github.com/google-research/bert)\n",
    "\n",
    "SpaCy's medium English model (with word vectors from GloVe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install extra models\n",
    "# only need to run this once per session\n",
    "#!python -m spacy download en_trf_bertbaseuncased_lg\n",
    "#!python -m spacy download en_core_web_md\n",
    "# you will likely need to restart your kernel to load the models\n",
    "#import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_trf_bertbaseuncased_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-5d50f68dd201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load BERT model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_trf_bertbaseuncased_lg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# load medium English model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_md\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_trf_bertbaseuncased_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# load BERT model\n",
    "nlp_bert = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
    "# load medium English model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp_bert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a844f6ff47c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# the spaCy Doc-Span-Token structure is still in place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This is a sentence.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnlp_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp_bert' is not defined"
     ]
    }
   ],
   "source": [
    "# the spaCy Doc-Span-Token structure is still in place\n",
    "text = \"This is a sentence.\"\n",
    "for model in [nlp_bert, nlp]:\n",
    "    print(model.meta['name'])\n",
    "    parsed = model(text)\n",
    "    print(type(parsed), type(parsed[0]))\n",
    "    # but the vector representation is different\n",
    "    print('Vector shape:', parsed.vector.shape)\n",
    "    # documents under the transformer model have additional attributes\n",
    "    print(parsed._.trf_last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp_bert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-b5a47eb02acd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mapple_org\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Apple sold fewer iPhones this quarter.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mapple_food\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Apple pie is delicious.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnlp_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Similarity between senses:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapple_org\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapple_food\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp_bert' is not defined"
     ]
    }
   ],
   "source": [
    "# adapted from spaCy's example\n",
    "# compare two different senses of the word \"Apple\" based on similarity\n",
    "# similarity: higher values = more similar\n",
    "apple_org = \"Apple sold fewer iPhones this quarter.\"\n",
    "apple_food = \"Apple pie is delicious.\"\n",
    "for model in [nlp_bert, nlp]:\n",
    "    print(model.meta['name'])\n",
    "    print('Similarity between senses:', model(apple_org)[0].similarity(model(apple_food)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional example: SciSpaCy\n",
    "I noticed a lot of people in the class are working in pharma and settings where you'd be dealing with scientific text.  When dealing with that, it might make sense to use a model trained on that sort of data.  Enter [scispaCy](https://allenai.github.io/scispacy/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you'll need to run these on collab\n",
    "#!pip install scispacy\n",
    "#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz\n",
    "# after this, you may need to restart the runtime\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sci = spacy.load(\"en_core_sci_sm\")\n",
    "nlp_web = spacy.load(\"en_core_web_sm\")\n",
    "text = \"\"\"\n",
    "Myeloid derived suppressor cells (MDSC) are immature \n",
    "myeloid cells with immunosuppressive activity. \n",
    "They accumulate in tumor-bearing mice and humans \n",
    "with different types of cancer, including hepatocellular \n",
    "carcinoma (HCC).\n",
    "\"\"\"\n",
    "doc = nlp_sci(text)\n",
    "print(doc.ents)\n",
    "doc = nlp_web(text)\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
