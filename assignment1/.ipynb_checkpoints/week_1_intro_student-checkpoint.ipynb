{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Week 1: Introduction and Overview\n",
    "This notebook accompanies the week 1 lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'spacy-transformers'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "import transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Natural Language Tool Kit vs SpaCy\n",
    "For this course, we will mainly be using [SpaCy]().  There are a [number of other]() NLP libraries and probably one of the best known is the [Natural Language Tool Kit (NLTK)]().  SpaCy and NLTK both are very powerful, but here I'll show a couple of reasons why I prefer SpaCy.\n",
    "\n",
    "Note: You will not be able to run these on your own without installing NLTK.  Since it's not used in the rest of the course, I'm not configuring it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# spacy\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "text = 'We are doing NLP.'\n",
    "doc = en(text)\n",
    "print(type(doc))\n",
    "print([(x, type(x)) for x in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "doc = word_tokenize(text)\n",
    "print(type(doc))\n",
    "print([(x, type(x)) for x in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time comparison\n",
    "%timeit en(text)\n",
    "%timeit nltk.tokenize.casual_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing: lower case and removing non-alpha\n",
    "text = 'We are doing NLP.'\n",
    "# spacy\n",
    "%timeit  [x.lower_ for x in en(text) if x.is_alpha]\n",
    "# nltk (one possible way)\n",
    "%timeit nltk.RegexpTokenizer(r'\\w+').tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You can see both libraries are very powerful.  But SpaCy's syntax is a bit simpler and it's generally a bit faster.  Have you tried NLTK or other libraries? Let's discuss.\n",
    "\n",
    "### SpaCy's language models\n",
    "We'll cover this more extensively in the slides.  But here's some illustration of what's going on under the hood with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at the different languages supported by spacy\n",
    "#from spacy.lang import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's in a language model\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "print(en.tokenizer)\n",
    "print(en.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in English model with tagging/entity pipeline components\n",
    "# you will need to run the line below beforehand\n",
    "#!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning text into a spacy document\n",
    "doc = en('We are doing NLP')\n",
    "print(doc)\n",
    "print(type(doc))\n",
    "doc_attrs = set(dir(doc))\n",
    "print(doc_attrs)\n",
    "# spans: subsets of doc\n",
    "print(doc[:2])\n",
    "print(type(doc[:2]))\n",
    "span_attrs = set(dir(doc[:2]))\n",
    "print(span_attrs - doc_attrs)\n",
    "# tokens: individual units of doc\n",
    "print(doc[0])\n",
    "print(type(doc[0]))\n",
    "print(dir(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internally, spacy represents tokens as hash values\n",
    "print(doc[0].lower)\n",
    "en.vocab[doc[0].lower].text\n",
    "# but you probably won't need that often\n",
    "print(doc[0].lower_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "This section shows some of the considerations to make when tokenizing your data.\n",
    "\n",
    "Token = \"Useful semantic unit\"\n",
    "\n",
    "But what does that mean? This section will detail some considerations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# importing different languages in spacy\n",
    "# blank English model\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "# blank Chinese model\n",
    "# to run, will need to install jieba tokenizer (optional)\n",
    "#!pip install jieba\n",
    "from spacy.lang.zh import Chinese\n",
    "\n",
    "zh = spacy.lang.zh.Chinese()\n",
    "zh_text = '我们正在做NLP。'\n",
    "print('Tokenize in Chinese:', [x.text for x in zh(zh_text)])\n",
    "print('Tokenize in English:', [x.text for x in en(zh_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting tidbit: the base Chinese model uses the jieba tokenizer\n",
    "zh.tokenizer\n",
    "# any other languages that might require a custom tokenization strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# lowercasing\n",
    "text = 'We are doing NLP.'\n",
    "print('Base python: ', text.lower())\n",
    "print('SpaCy:', [x.lower_ for x in en(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# handling non-alpha\n",
    "text = 'We are doing NLP.'\n",
    "# base python\n",
    "strip_punct = '[^A-Za-z0-9 ]'\n",
    "print(re.sub(strip_punct, '', text))\n",
    "# spacy\n",
    "print([x.text for x in en(text) if x.is_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but what about contractions?\n",
    "text = \"We're doing NLP.\"\n",
    "# base python\n",
    "strip_punct = '[^A-Za-z0-9 ]'\n",
    "print('Just removing punctuation:', re.sub(strip_punct, '', text))\n",
    "# spacy\n",
    "print('Removing non-alpha', [x.text for x in en(text) if x.is_alpha])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the is_alpha flag is False for any tokens that have non-alpha characters.  We'll look into a better way for dealing with contractions later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a tokenizer\n",
    "In this exercise, you will make a function that uses spaCy's base English model to tokenize a dataset according to specific parameters.  The functions will take a list of documents and output a list of tokens.  In this case we're interested in outputting strings, rather than spaCy tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming\n",
    "Though word tense can sometimes carry with it a lot of useful information, a lot of time it might be useful to reduce words to their common root.  For example, the word \"be\" has various forms like \"are\", \"is\", \"been\".  We might not want our vocabulary to contain all these forms and rather count them all as instances of \"be\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in English model with tagging/entity pipeline components\n",
    "# you will need to run the line below beforehand\n",
    "#!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'I am taking an NLP course.'\n",
    "print(text)\n",
    "print([x.lemma_ for x in nlp(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guess how lemmatization might change this\n",
    "text = 'We are recording this class with Zoom.'\n",
    "guess = ''\n",
    "print(text)\n",
    "print([x.lemma_ for x in nlp(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "Dealing with stop words involves making some pretty impactful decisions with your data.  Refer to the slides for details.  Here, we just remove stop words based on [spaCy's default set](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = English()\n",
    "text = 'In June 2020, I took a course at Harvard Extension School in Cambridge.'\n",
    "print(text)\n",
    "print([x.text for x in en(text) if not x.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-standard tokens (e.g. named-entities)\n",
    "In text, some some n-grams should not be treated as a concatenation of unigrams.  For example, New York City is fundamentally different from the individual words \"new\", \"york\" and \"city\".\n",
    "\n",
    "Here we attempt to deal with some of these non-standard tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls\n",
    "# base python\n",
    "# regex from textacy: https://github.com/chartbeat-labs/textacy\n",
    "SHORT_URL_REGEX = re.compile(\n",
    "    r\"(?:^|(?<![\\w/.]))\"\n",
    "    # optional scheme\n",
    "    r\"(?:(?:https?://)?)\"\n",
    "    # domain\n",
    "    r\"(?:\\w-?)*?\\w+(?:\\.[a-z]{2,12}){1,3}\"\n",
    "    r\"/+\",\n",
    "    flags=re.IGNORECASE)\n",
    "text = 'Check out these courses: https://www.summer.harvard.edu/'\n",
    "print(text)\n",
    "print(SHORT_URL_REGEX.sub('', text))\n",
    "# spacy\n",
    "print([x for x in en(text) if not x.like_url])\n",
    "# spacy - replace with a standard token\n",
    "print(['-URL-' if x.like_url else x for x in en(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# named-entities\n",
    "# read in English model with tagging/entity pipeline components\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = 'I am taking an NLP course at Harvard starting July 19th, 2020'\n",
    "parsed = nlp(text)\n",
    "# look at the individual tokens\n",
    "tokens = [t for t in parsed]\n",
    "print(tokens)\n",
    "# look at the identified named-entities and their types\n",
    "for e in parsed.ents:\n",
    "    print(e, type(e), e.label_, spacy.explain(e.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: A comprehensive tokenization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counts\n",
    "A very basic way to use a sanitized list of tokens is to do a word count.  This unlocks a lot of insights right off and is an important step in exploratory data analysis in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "\n",
    "def simple_tokenizer(doc, model=en):\n",
    "    # a simple tokenizer for individual documents (different from above)\n",
    "    tokenized_docs = []\n",
    "    parsed = model(doc)\n",
    "    return([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "text_data = [\"I'm taking a course at Harvard.\",\n",
    "            \"I'm learning about Natural Language Processing.\",\n",
    "            \"We are studying tokenization, vectorization and modelling.\",\n",
    "            \"Check out the course on Github: https://github.com/bpben/nlp_lessons\"]\n",
    "tokenized = [simple_tokenizer(doc) for doc in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base python: create an make use of a Counter object\n",
    "counts = [Counter(d) for d in tokenized]\n",
    "print('List of counts:', counts)\n",
    "# sum together all counts\n",
    "all_counts = Counter()\n",
    "for d in tokenized:\n",
    "    all_counts += Counter(d)\n",
    "print(counts)\n",
    "print('\\nCombined count:', all_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn's countvectorizer\n",
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our custom tokenizer\n",
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "# outputs sparse array, want to use a normal numpy array\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "# get_feature_names gets the vocabulary of the vectorizer in order\n",
    "dict(zip(cv.get_feature_names(), v.sum(axis=0)))\n",
    "# result is the same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other neat uses of CountVectorizer\n",
    "# turning it into a dataframe for easier manipulation\n",
    "cv = CountVectorizer()\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "pd.DataFrame(v, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# character-level\n",
    "cv = CountVectorizer(analyzer='char')\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "pd.DataFrame(v, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-grams\n",
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "pd.DataFrame(v, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-specified vocabulary\n",
    "cv = CountVectorizer(vocabulary=['natural', 'language', 'processing', 'harvard'])\n",
    "v = cv.fit_transform(text_data).toarray()\n",
    "pd.DataFrame(v, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Sentiment analysis with word counts\n",
    "Imagine you are a hot dog restaurant owner and you want to analyze a corpus of reviews from diners to see whether people generally think your hot dogs are \"good\" or \"bad\".  Specifically, you're going to count up the number of times the word \"good\" and word \"bad\" appears.  Depending on how you process the text, you will arrive at different conclusions.  Try a couple ways to see what I mean.\n",
    "\n",
    "You might also want to think about whether all the reviews are relevant.  Those sorts of choices may also affect your results.  Is there an automatic way you can remove non-relevant reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['These hot dogs are really good.',\n",
    "          'These hot dogs are really bad.',\n",
    "          'Good hot dogs!',\n",
    "          'The hot dogs pair well with a Good Humor bar.',\n",
    "          \"I didn't eat anything, I felt bad.\",\n",
    "          \"I had a good time!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple statistical test: Log-likelihood ratio\n",
    "Above we just compared the count of the word good to the count of the word bad.  But we can actually test the significance of this difference using a test of log-likelihood ratio.  Refer to the slides for a bit more information, but here's a calculation based on the above example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log, mean\n",
    "def log_likelihood(analysis, reference, word):\n",
    "    # count of word in source\n",
    "    a = analysis[word].sum()\n",
    "    # count of word in reference\n",
    "    b = reference[word].sum()\n",
    "    # count of all words in source\n",
    "    c = analysis.sum().sum()\n",
    "    # count of all words in reference\n",
    "    d = reference.sum().sum()\n",
    "    print('counts analysis:', a)\n",
    "    print('counts reference:', b)\n",
    "    e1 = c*(a+b)/(c+d)\n",
    "    e2 = d*(a+b)/(c+d)\n",
    "    g = 2*((a*log(a/e1)) + (b*log(b/e2)))\n",
    "    print('G2: ', g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis: hot dog texts\n",
    "analysis = count_df[(count_df['hot'] + count_df['dogs'])==2]\n",
    "print(analysis.shape)\n",
    "# reference: non hot dog texts\n",
    "reference = count_df[(count_df['hot'] + count_df['dogs'])!=2]\n",
    "print(reference.shape)\n",
    "log_likelihood(analysis, reference, 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is pretty non-significant\n",
    "# G2=3.84 means 5% of getting this difference by chance, this is closer to 30%\n",
    "# if we added a few to analysis\n",
    "analysis['good'] = analysis['good']+3\n",
    "log_likelihood(analysis, reference, 'good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to advanced models\n",
    "In this section, we'll be setting up some of the requirements for the more advanced techniques we will cover later in the course.  Particularly, we'll be working with:\n",
    "\n",
    "- [huggingface's transformers library](https://github.com/huggingface/transformers)\n",
    "- [spaCy-transformers (based on the above)](https://github.com/explosion/spacy-transformers)\n",
    "\n",
    "These require some additional downloads.  For these examples you'll need:\n",
    "\n",
    "[BERT uncased large model](https://github.com/google-research/bert)\n",
    "\n",
    "SpaCy's medium English model (with word vectors from GloVe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install extra models\n",
    "# only need to run this once per session\n",
    "#!python -m spacy download en_trf_bertbaseuncased_lg\n",
    "#!python -m spacy download en_core_web_md\n",
    "# you will likely need to restart your kernel to load the models\n",
    "#import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BERT model\n",
    "nlp_bert = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
    "# load medium English model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the spaCy Doc-Span-Token structure is still in place\n",
    "text = \"This is a sentence.\"\n",
    "for model in [nlp_bert, nlp]:\n",
    "    print(model.meta['name'])\n",
    "    parsed = model(text)\n",
    "    print(type(parsed), type(parsed[0]))\n",
    "    # but the vector representation is different\n",
    "    print('Vector shape:', parsed.vector.shape)\n",
    "    # documents under the transformer model have additional attributes\n",
    "    print(parsed._.trf_last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adapted from spaCy's example\n",
    "# compare two different senses of the word \"Apple\" based on similarity\n",
    "# similarity: higher values = more similar\n",
    "apple_org = \"Apple sold fewer iPhones this quarter.\"\n",
    "apple_food = \"Apple pie is delicious.\"\n",
    "for model in [nlp_bert, nlp]:\n",
    "    print(model.meta['name'])\n",
    "    print('Similarity between senses:', model(apple_org)[0].similarity(model(apple_food)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional example: SciSpaCy\n",
    "I noticed a lot of people in the class are working in pharma and settings where you'd be dealing with scientific text.  When dealing with that, it might make sense to use a model trained on that sort of data.  Enter [scispaCy](https://allenai.github.io/scispacy/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you'll need to run these on collab\n",
    "#!pip install scispacy\n",
    "#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz\n",
    "# after this, you may need to restart the runtime\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sci = spacy.load(\"en_core_sci_sm\")\n",
    "nlp_web = spacy.load(\"en_core_web_sm\")\n",
    "text = \"\"\"\n",
    "Myeloid derived suppressor cells (MDSC) are immature \n",
    "myeloid cells with immunosuppressive activity. \n",
    "They accumulate in tumor-bearing mice and humans \n",
    "with different types of cancer, including hepatocellular \n",
    "carcinoma (HCC).\n",
    "\"\"\"\n",
    "doc = nlp_sci(text)\n",
    "print(doc.ents)\n",
    "doc = nlp_web(text)\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
