{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Week 4: Attention and Transformers\n",
    "This notebook accompanies the week 4 lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'numpy', \n",
    "            'pandas', 'torch', 'matplotlib',\n",
    "            'transformers', 'allennlp==0.9.0'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import transformers\n",
    "\n",
    "from spacy.lang.en import English\n",
    "# !python -m spacy download en_core_web_md\n",
    "# import en_core_web_md\n",
    "en = English()\n",
    "# nlp = en_core_web_md.load()\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# this will set the device on which to train\n",
    "device = torch.device(\"cpu\")\n",
    "# if using collab, set your runtime to use GPU and use the line below\n",
    "#device = torch.device(\"cuda:0\")\n",
    "\n",
    "def simple_tokenizer(doc, model=en):\n",
    "    # a simple tokenizer for individual documents (different from above)\n",
    "    tokenized_docs = []\n",
    "    parsed = model(doc)\n",
    "    return([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])\n",
    "\n",
    "def pad_sequence(seqs, seq_len=200):\n",
    "    # function for adding padding to ensure all seq same length\n",
    "    features = np.zeros((len(seqs), seq_len),dtype=int)\n",
    "    for i, seq in enumerate(seqs):\n",
    "        if len(seq) != 0:\n",
    "            features[i, -len(seq):] = np.array(seq)[:seq_len]\n",
    "    return features\n",
    "\n",
    "def doc_to_index(docs, vocab):\n",
    "    # transform docs into series of indices\n",
    "    docs_idxs = []\n",
    "    for d in docs:\n",
    "        w_idxs = []\n",
    "        for w in d:\n",
    "            if w in vocab:\n",
    "                w_idxs.append(vocab[w])\n",
    "            else:\n",
    "                # unknown token = 1\n",
    "                w_idxs.append(1)\n",
    "        docs_idxs.append(w_idxs)\n",
    "    return(docs_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 review tidbit\n",
    "We should always be careful with our decompositions.  If they're using different features, fit on different data slices, they're not going to be directly comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "X, _ = make_blobs(n_samples=100, centers=1, n_features=10,\n",
    "                  random_state=0)\n",
    "# take a random subsample\n",
    "sub_idxs = np.where(np.random.random(len(X))<=0.25)\n",
    "sub_X = X[sub_idxs]\n",
    "print(sub_idxs)\n",
    "# we're just going to pick a couple idxs and force them close\n",
    "sampled_idxs = sub_idxs[0][:2]\n",
    "X[sampled_idxs[0]] = X[sampled_idxs[1]]+0.1\n",
    "\n",
    "pca_full = PCA(n_components=2)\n",
    "pca_sub = PCA(n_components=2)\n",
    "X_pca = pca_full.fit_transform(X)\n",
    "X_pca_sub = pca_sub.fit_transform(sub_X)\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, sharex=True)\n",
    "axs[0].scatter(X_pca[:,0], X_pca[:,1])\n",
    "axs[0].scatter(X_pca[sampled_idxs,0], X_pca[sampled_idxs,1], color='r')\n",
    "axs[1].scatter(X_pca_sub[:,0], X_pca_sub[:,1])\n",
    "axs[1].scatter(X_pca_sub[:2,0], X_pca_sub[:2,1], color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General data formatting\n",
    "Usually, you'd want a set of functions to carry out most of this.  But for now, we're just including it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will need to change this to where ever the file is stored\n",
    "data_location = './data/assignment_1_reviews.pkl'\n",
    "with open(data_location, 'rb') as f:\n",
    "    all_text = pickle.load(f)\n",
    "neg, pos = all_text.values()\n",
    "# join all reviews\n",
    "all_reviews = np.array(neg+pos)\n",
    "# create binary indicator for positive review\n",
    "is_positive = np.array([0]*len(neg)+[1]*len(pos))\n",
    "# set the seed for numpy\n",
    "np.random.seed(seed=42)\n",
    "# shuffle, just for safety\n",
    "shuffled_idxs = np.random.choice(range(len(all_reviews)), size=len(all_reviews),replace=False)\n",
    "all_reviews = all_reviews[shuffled_idxs]\n",
    "is_positive = is_positive[shuffled_idxs]\n",
    "# sample random 70% for fitting model (training)\n",
    "# we'll also add a validation set, for checking the progress of the model during training\n",
    "# 30% will be simulating \"new observations\" (testing)\n",
    "pct_train = 0.7\n",
    "train_bool = np.random.random(len(all_reviews))<=pct_train\n",
    "reviews_train = all_reviews[train_bool]\n",
    "reviews_test = all_reviews[~train_bool]\n",
    "is_positive_train = is_positive[train_bool]\n",
    "is_positive_test = is_positive[~train_bool]\n",
    "# making a validation set\n",
    "pct_val = 0.3\n",
    "val_idxs = np.random.random(size=len(reviews_train))<=pct_val\n",
    "is_positive_val = is_positive_train[val_idxs]\n",
    "is_positive_val.shape\n",
    "reviews_val = reviews_train[val_idxs]\n",
    "# reconfigure train so that it doesn't include validation\n",
    "reviews_train = reviews_train[~val_idxs]\n",
    "is_positive_train = is_positive_train[~val_idxs]\n",
    "print('Train: {0} \\nValidation: {1} \\nTest: {2}'.format(\n",
    "    len(reviews_train), len(reviews_val), len(reviews_test)))\n",
    "parsed_train = [simple_tokenizer(str(d)) for d in reviews_train]\n",
    "parsed_val = [simple_tokenizer(str(d)) for d in reviews_val]\n",
    "parsed_test = [simple_tokenizer(str(d)) for d in reviews_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct glove weight matrix\n",
    "# construct vocab\n",
    "cv = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False, min_df=0.01)\n",
    "cv.fit(parsed_train)\n",
    "vocab = cv.vocabulary_\n",
    "print(\"Size of vocab:\", len(vocab))\n",
    "vocab = cv.vocabulary_\n",
    "vocab = dict([(v, vocab[v]+2) for v in vocab])\n",
    "vocab['_UNK'] = 1\n",
    "vocab['_PAD'] = 0\n",
    "glove_vecs = np.zeros(shape=(len(vocab), 300))\n",
    "for k, v in vocab.items():\n",
    "    glove_vecs[v] = nlp(k).vector\n",
    "# additional formatting\n",
    "idx_train = doc_to_index(parsed_train, vocab)\n",
    "padded_train = pad_sequence(idx_train)\n",
    "idx_val = doc_to_index(parsed_val, vocab)\n",
    "padded_val = pad_sequence(idx_val)\n",
    "idx_test = doc_to_index(parsed_test, vocab)\n",
    "padded_test = pad_sequence(idx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-directional LSTMs and ELMo\n",
    "ELMo's architecture extends on what we've already learned about LSTM models.  ELMo uses a bi-directional LSTM, which is simply two connected LSTMs that step through each observation in opposite directions.  Incidentally, we can adapt our existing LSTM model to be bi-direction very easily.  In our SentimentNet model, just replace the line:\n",
    "\n",
    "`self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout_prob, batch_first=True)`\n",
    "\n",
    "with \n",
    "\n",
    "`self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout_prob, batch_first=True, bidirectional=True)`\n",
    "\n",
    "We won't implement that here, but feel free to try it on your own time.\n",
    "\n",
    "ELMo, unlike our SentimentNet, is trained as a \"language model\".  That means its objective is to predict the next word given the context.  So at time t, it combines the representation from t-1 (the forward LSTM) and the representation from t+1 (the backward LSTM) to predict the word.  The representations from this model, trained on the language modelling task, can then be used as contextualized word-level embeddings.\n",
    "\n",
    "For this example, we'll look at a case of [polysemy](https://en.wikipedia.org/wiki/Polysemy), where the word \"stick\" can mean \"keep with\" or a piece of a tree, depending on context.\n",
    "\n",
    "The `allennlp` library has a great, simple implementation of ELMo for transfer learning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"Let's stick to the script\",\n",
    "       \"I threw the stick to the dog\",\n",
    "       \"We should stick together\"]\n",
    "\n",
    "# elmo embedder expects a list of tokens\n",
    "token_docs = [simple_tokenizer(d) for d in docs]\n",
    "elmo_vecs = [elmo.embed_sentence(d) for d in token_docs]\n",
    "elmo_vecs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `ElmoEmbedder` contains three vectors of length 1024 for each word.  From [the paper](https://arxiv.org/pdf/1802.05365.pdf), the architecture of the model is an embedding layer and two Bi-directional LSTMs (two sets of forward and backward).  So, we have three representations of each word.\n",
    "\n",
    "There's learning incorporated into each of these layers, but we'll be using the last layer.  Other implementations of ELMo use a weighted combination of all three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first document in corpus\n",
    "first_doc = elmo_vecs[0]\n",
    "print(first_doc.shape)\n",
    "# last layer\n",
    "first_doc_last_layer = first_doc[-1]\n",
    "print(first_doc_last_layer.shape)\n",
    "# first word's embedding (\"Let\")\n",
    "print(first_doc_last_layer[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Polysemy in GloVe and ELMo\n",
    "Using GloVe and ELMo, get the embeddings for the word \"stick\" in different contexts (i.e. doc 1 and 3 vs doc 2).  Compare their cosine similarity and interpret.\n",
    "\n",
    "If you finish this quickly, see if you can come up with other examples to test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = -1\n",
    "word = 'stick'\n",
    "elmo_word_vecs = [s[layer_idx][token_docs[i].index(word)] for i, s in enumerate(elmo_vecs)]\n",
    "cosine_similarity(elmo_word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that doc 1 and doc 3 that use \"stick\" in the \"keep with\" sense have higher similarity than doc 2, which uses the \"piece of tree\" context.\n",
    "\n",
    "With GloVe, each word is considered without context.  So stick in doc 1 and doc 2 have the same vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_docs = [nlp(d) for d in docs]\n",
    "print(parsed_docs[0][2])\n",
    "print(parsed_docs[2][2])\n",
    "cosine_similarity(\n",
    "    parsed_docs[0][2].vector.reshape(1,-1),\n",
    "    parsed_docs[1][2].vector.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trade-off here is it's not so easy to just create a ELMo word embedding vector, because a word's embedding depends on its context.  Feel free to try it out in our SentimentNet model, but remember this requires running all reviews through ELMo and creating a much larger embedding matrix.\n",
    "\n",
    "ELMo is still a recurrent model.  We talked about some issues that face recurrent models such as vanishing gradients.  LSTMs solve that to some extent and the bi-directional nature of ELMo also addresses that.  \n",
    "\n",
    "But with large documents, context is pretty widely distributed.  LSTMs, no matter how they're shaped, are going to be biased towards nearby information.  We need some way to make sure, for each token, we're incorporating information relevant to THAT token, even if that information occurs many tokens in the past.  One way to do that:\n",
    "\n",
    "## Attention\n",
    "\n",
    "There's more detail about attention in the slides, but briefly, attention is a name for a set of techniques that weight certain inputs to increase signal and decrease noise. \n",
    "\n",
    "We'll be focusing on self-attention, in which we are weighting components of a particular sequence relative to individaul components of that same sequence.  With text that typically means having our document representation incorporate information not just from its neighbors, but from the entire document.\n",
    "\n",
    "We'll be taking our existing SentimentNet model and incorporating a simple implementation of self-attention.  In this case, we're weighting the hidden states by the dot product of the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def forward(self, embeds, hidden):\n",
    "        # getting the shape information\n",
    "        batch_size, output_len, dimensions = embeds.size()\n",
    "        # attention scores\n",
    "        attention_scores = torch.bmm(embeds, embeds.transpose(1, 2).contiguous())\n",
    "        attention_scores = attention_scores.view(batch_size * output_len, output_len)\n",
    "        # normalize\n",
    "        attention_weights = nn.functional.softmax(attention_scores)\n",
    "        attention_weights = attention_weights.view(batch_size, output_len, output_len)\n",
    "        # weight hidden layer\n",
    "        mix = torch.bmm(attention_weights, hidden)\n",
    "        # output result\n",
    "        return mix, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    # sentiment classifier with single LSTM layer + Fully-connected layer, sigmoid activation and dropout\n",
    "    # adapted from https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/\n",
    "    def __init__(self,\n",
    "                 weight_matrix=None,\n",
    "                 vocab_size=None, \n",
    "                 output_size=1,  \n",
    "                 hidden_dim=512,\n",
    "                 embedding_dim=400, \n",
    "                 n_layers=1, # mistake on previous notebooks, just 1 layer here \n",
    "                 dropout_prob=0.5):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        # size of the output, in this case it's one input to one output\n",
    "        self.output_size = output_size\n",
    "        # number of layers (default 2) one LSTM layer, one fully-connected layer\n",
    "        self.n_layers = n_layers\n",
    "        # dimensions of our hidden state, what is passed from one time point to the next\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # initialize the representation to pass to the LSTM\n",
    "        self.embedding, embedding_dim = self.init_embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            weight_matrix)\n",
    "        # LSTM layer, where the magic happens\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=dropout_prob, batch_first=True)\n",
    "        # dropout, similar to regularization\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        # sigmoid activiation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # adding attention layer\n",
    "        self.attn = Attention(hidden_dim)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # forward pass of the network\n",
    "        batch_size = x.size(0)\n",
    "        # transform input\n",
    "        embeds = self.embedding(x)\n",
    "        # run input embedding + hidden state through model\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # implementing attention\n",
    "        weighted_out, attention_weights = self.attn(embeds, lstm_out)\n",
    "        # using weighted output intstead of lstm output\n",
    "        # reshape\n",
    "        weighted_out = weighted_out.contiguous().view(-1, self.hidden_dim)\n",
    "        # dropout certain pct of connections\n",
    "        out = self.dropout(weighted_out)\n",
    "        # fully connected layer\n",
    "        out = self.fc(out)\n",
    "        # activation function\n",
    "        out = self.sigmoid(out)\n",
    "        # reshape\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        # return the output and the hidden state\n",
    "        return out, hidden, attention_weights, lstm_out\n",
    "    \n",
    "    def init_embedding(self, vocab_size, embedding_dim, weight_matrix):\n",
    "        # initializes the embedding\n",
    "        if weight_matrix is None:\n",
    "            if vocab_size is None:\n",
    "                raise ValueError('If no weight matrix, need a vocab size')\n",
    "            # if embedding is a size, initialize trainable\n",
    "            return(nn.Embedding(vocab_size, embedding_dim),\n",
    "                   embedding_dim)\n",
    "        else:\n",
    "            # otherwise use matrix as pretrained\n",
    "            weights = torch.FloatTensor(weight_matrix)\n",
    "            return(nn.Embedding.from_pretrained(weights),\n",
    "                  weights.shape[1])\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # initializes the hidden state\n",
    "        hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device),\n",
    "                  torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'weight_matrix': glove_vecs,\n",
    "               'output_size': 1,\n",
    "               'hidden_dim': 512,\n",
    "               'n_layers': 1,\n",
    "               'embedding_dim': 400,\n",
    "               'dropout_prob': 0.20}\n",
    "training_params = {'learning_rate': 0.005,\n",
    "                  'epochs': 1,\n",
    "                  'batch_size': 100}\n",
    "batch_size = training_params['batch_size']\n",
    "# construct datasets for loading by PyTorch\n",
    "train_data = TensorDataset(torch.from_numpy(padded_train), torch.from_numpy(is_positive_train))\n",
    "val_data = TensorDataset(torch.from_numpy(padded_val), torch.from_numpy(is_positive_val))\n",
    "test_data = TensorDataset(torch.from_numpy(padded_test), torch.from_numpy(is_positive_test))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,\n",
    "                         drop_last=True) # this is to keep the size consistent\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size,\n",
    "                       drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,\n",
    "                        drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentNet(**model_params)\n",
    "model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                         lr=training_params['learning_rate'])\n",
    "# increasing this will make the training take a while on CPU\n",
    "# decrease to 5 if it's taking too long\n",
    "epochs = training_params['epochs']\n",
    "batch_size = training_params['batch_size']\n",
    "counter = 0\n",
    "print_every = 5\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h, train_attention_weights, lstm_out = model(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h, val_attention_weights, _ = model(inp, val_h)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the weights, for just one observation\n",
    "# what dimensions do we expect? (attention is with respect to all other states in sequence)\n",
    "#train_attention_weights[0].shape\n",
    "# knowing that our weights are just a dot-product of the word vectors...what should they be?\n",
    "# specifically, where will be the highest weights?\n",
    "# I'm making a useful viz here, but you don't need to wrangle with seaborn\n",
    "#!pip install seaborn\n",
    "import seaborn as sns\n",
    "sns.heatmap(train_attention_weights[0].detach().cpu())\n",
    "#np.argmax(train_attention_weights[0].detach().cpu(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Encoder Representations from Transformers (BERT)\n",
    "In 2018, researchers from Google released a [landmark paper](https://arxiv.org/pdf/1810.04805.pdf) that represented a major advancement in NLP.  It incorporated the concepts of Encoder-Decoder architectures with Attention to create an architecture that outperformed the state-of-the-art on a variety of tasks.  \n",
    "\n",
    "One of the most powerful features of BERT is that it can be pre-trained on a set of tasks that don't require labelled data.  One of them is the language modelling task we've discussed before (i.e. predicting the next word).  The other is predicting whether one sentence follows another.  The model, trained on these two tasks, can produce representations that incorporate both syntactic and semantic information.\n",
    "\n",
    "We'll be using the `transformers` library from [HuggingFace](https://huggingface.co/transformers/) to leverage a pre-trained version of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic pretrained model (case-insensitive)\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "# Load pre-trained model\n",
    "model = BertModel.from_pretrained(MODEL_NAME)\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/huggingface/transformers/blob/master/notebooks/02-transformers.ipynb\n",
    "# tokenize according to BERT tokenization design\n",
    "tokens = tokenizer.tokenize(\"This is an input example\")\n",
    "print(\"Tokens: {}\".format(tokens))\n",
    "\n",
    "# transform to ids for model\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens id: {}\".format(tokens_ids))\n",
    "\n",
    "# Add the required special tokens ([CLS] and [SEP])\n",
    "tokens_ids = tokenizer.build_inputs_with_special_tokens(tokens_ids)\n",
    "print(\"Tokens id: {}\".format(tokens_ids))\n",
    "print(\"Tokens + special tokens: {}\".format(\n",
    "    tokenizer.convert_ids_to_tokens(tokens_ids)))\n",
    "\n",
    "# convert to tensor (PyTorch)\n",
    "tokens_pt = torch.tensor([tokens_ids])\n",
    "\n",
    "# Now we're ready to go through BERT with out input\n",
    "outputs, pooled = model(tokens_pt)\n",
    "print(\"Token wise output: {}, Pooled output: {}\".format(outputs.shape, pooled.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model gives two outputs: a token-level representation of length 768 (i.e. one 768-element representation per token) and a sentence-level representation of length 768.\n",
    "\n",
    "Let's return to the example above and see how the word-level representation deals with polysemy.\n",
    "\n",
    "Note: `BertTokenizer` has a method that implements all of the tokenization steps above in one line for multiple sentences/documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"Let's stick to the script\",\n",
    "       \"I threw the stick to the dog\",\n",
    "       \"We should stick together\"]\n",
    "\n",
    "tokens = tokenizer.batch_encode_plus(docs,\n",
    "    pad_to_max_length=True, # this implements padding for different length docs\n",
    "    return_tensors=\"pt\") # returning as pytorch tensors\n",
    "\n",
    "outputs, pooled = model(**tokens)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'stick'\n",
    "token_id = tokenizer.convert_tokens_to_ids('stick')\n",
    "token_idx = np.where(tokens['input_ids']==6293)[1]\n",
    "bert_word_vecs = [d[token_idx[i]].detach().numpy() for i, d in enumerate(outputs)]\n",
    "cosine_similarity(bert_word_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to ELMo, the similarity between doc 1 and doc 3's embedding for \"stick\" is higher than doc 1 and doc 2's embedding for \"stick.\n",
    "\n",
    "Let's make use of the document-level embedding.  Let's take a random set of negative and positive reviews and look at the similarity within and between classes.\n",
    "\n",
    "### Exercise: Intra- and inter-class similarity using BERT\n",
    "\n",
    "Sample five negative and five positive reviews.  Run them through `tokenizer.batch_encode_plus`.  Then take the \"pooled\" (document-level) representation and run cosine similarity to get the inter-class and intra-class similarities.\n",
    "\n",
    "You may get some warnings! Read them, and think about why they're appearing.\n",
    "\n",
    "Note: The reason you're taking only 10 reviews is because BERT takes a bit to run.  Feel free to take a larger sample if you have time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random set of neg/pos\n",
    "neg_sample = np.random.choice(neg, size=5)\n",
    "pos_sample = np.random.choice(pos, size=5)\n",
    "full_sample = np.concatenate([neg_sample, pos_sample])\n",
    "tokens = tokenizer.batch_encode_plus(full_sample,\n",
    "    pad_to_max_length=True, # this implements padding for different length docs\n",
    "    return_tensors=\"pt\") # returning as pytorch tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random set of neg/pos\n",
    "np.random.seed(42)\n",
    "neg_sample = np.random.choice(neg, size=5)\n",
    "pos_sample = np.random.choice(pos, size=5)\n",
    "full_sample = np.concatenate([neg_sample, pos_sample])\n",
    "tokens = tokenizer.batch_encode_plus(full_sample,\n",
    "    pad_to_max_length=True, # this implements padding for different length docs\n",
    "    return_tensors=\"pt\", # returning as pytorch tensors\n",
    "    max_length=512) # this BERT model has a max sequence length of 512 tokens\n",
    "\n",
    "outputs, pooled = model(**tokens)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vecs = pooled.detach().numpy()\n",
    "bert_sims = cosine_similarity(bert_vecs)\n",
    "print('Neg to neg:',(bert_sims[:5, :5]).mean().mean())\n",
    "print('Neg to pos:',(bert_sims[:5, 5:]).mean().mean())\n",
    "print('Pos to pos:',(bert_sims[5:, 5:]).mean().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! So negative reviews' review-level representations are more similar to other negative reviews and less similar to poisitive reviews.  The same is true for positive reviews, though the difference is not as large.  Of course, this is a small subset of the complete data, so it's hard to draw conclusions.  But this gives you a method for making use of the document-level representations from pretrained BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic pretrained model (case-insensitive)\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "# Load pre-trained model\n",
    "model = DistilBertModel.from_pretrained(MODEL_NAME)\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random set of neg/pos\n",
    "neg_sample = np.random.choice(neg, size=5)\n",
    "pos_sample = np.random.choice(pos, size=5)\n",
    "full_sample = np.concatenate([neg_sample, pos_sample])\n",
    "tokens = tokenizer.batch_encode_plus(full_sample,\n",
    "    pad_to_max_length=True, # this implements padding for different length docs\n",
    "    return_tensors=\"pt\") # returning as pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 0\n",
    "# batch-wise, otherwise tends to take a long time\n",
    "batches = list(range(100, len(data), 100))+[len(data)]\n",
    "doc_rep_collector = []\n",
    "for b in batches:\n",
    "    tokens = tokenizer.batch_encode_plus(data['text'][st:b],\n",
    "      pad_to_max_length=True, return_tensors=\"pt\")\n",
    "    st = b\n",
    "    outputs = model(**tokens)\n",
    "    # taking the representation of the 'CLS' token (doc-level embedding)\n",
    "    doc_rep_collector.append(outputs[0][:,0].detach().numpy())\n",
    "    break\n",
    "doc_rep_collector = np.concatenate(doc_rep_collector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
