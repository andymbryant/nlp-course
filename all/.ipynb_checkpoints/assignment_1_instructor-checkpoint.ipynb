{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Tokenization and Word counts for sentiment analysis\n",
    "In this assignment, you will be applying the techniques learned in week 1 of the course to perform and analyze sentiment on a dataset of movie reviews from IMDB.\n",
    "\n",
    "This dataset comes from [Mass et. al. (2011)](https://www.aclweb.org/anthology/P11-1015.pdf) and the full version is available [here](http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from collections import Counter\n",
    "import re\n",
    "from numpy import log, mean\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'pandas', 'transformers'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/assignment_1_reviews.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-574b017ad252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mpct_sample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mall_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/assignment_1_reviews.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/assignment_1_reviews.pkl'"
     ]
    }
   ],
   "source": [
    "## NOTE: Below is just for reference for how I generated the data\n",
    "## if you run this, it will not work!\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "pct_sample = 0.1\n",
    "all_text = {}\n",
    "for p in ['neg', 'pos']:\n",
    "    all_text[p] = []\n",
    "    for f in glob('/Users/batorsky/Downloads/aclImdb/test/%s/*.txt' % p):\n",
    "        if np.random.rand()<=pct_sample:\n",
    "            all_text[p].append(open(f, encoding='utf-8').read())\n",
    "with open('../data/assignment_1_reviews.pkl', 'wb') as f:\n",
    "    pickle.dump(all_text, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "\n",
    "I've already processed the full dataset for you and saved it as a data file: `assignment_1_reviews.pkl`.  You don't need to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neg', 1233), ('pos', 1266)]\n"
     ]
    }
   ],
   "source": [
    "# you will need to change this to where ever the file is stored\n",
    "# on colab, you can likely just put this as 'assignment_1_reviews.pkl'\n",
    "data_location = './data/assignment_1_reviews.pkl'\n",
    "with open(data_location, 'rb') as f:\n",
    "    all_text = pickle.load(f)\n",
    "# corpora size\n",
    "print([(k, len(all_text[k])) for k in all_text])\n",
    "# for simplicity, let's split these into separate sets\n",
    "neg, pos = all_text.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Use what you've developed in the week 1 notebook to tokenize each of the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "\n",
    "def simple_tokenizer(doc, model=en):\n",
    "    # a simple tokenizer for individual documents (different from above)\n",
    "    tokenized_docs = []\n",
    "    parsed = model(doc)\n",
    "    return([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_neg = [simple_tokenizer(x) for x in neg]\n",
    "token_pos = [simple_tokenizer(x) for x in pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counts\n",
    "Create a count of the number of words in each review.  Use scikit-learn's CountVectorizer.  Refer to the documentation as it has a few parameters you might want to think about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "# should probably fit on the combined\n",
    "cv.fit(neg+pos)\n",
    "count_neg = cv.transform(neg).toarray()\n",
    "count_pos = cv.transform(pos).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use pandas DF here\n",
    "neg_df = pd.DataFrame(count_neg, columns=cv.get_feature_names())\n",
    "split_pos = len(neg_df)\n",
    "pos_df = pd.DataFrame(count_pos, columns=cv.get_feature_names())\n",
    "# combine for more ease\n",
    "all_df = pos_df.append(neg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent words\n",
    "What are the top 10 most frequent words in the positive reviews? The negative reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>7897.0</td>\n",
       "      <td>7784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>8677.0</td>\n",
       "      <td>7206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>3834.0</td>\n",
       "      <td>4357.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>4971.0</td>\n",
       "      <td>4325.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>5643.0</td>\n",
       "      <td>5102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>4445.0</td>\n",
       "      <td>4462.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>7758.0</td>\n",
       "      <td>6586.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>3425.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>16801.0</td>\n",
       "      <td>15746.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3918.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>6501.0</td>\n",
       "      <td>6937.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        True     False\n",
       "a      7897.0   7784.0\n",
       "and    8677.0   7206.0\n",
       "i      3834.0   4357.0\n",
       "in     4971.0   4325.0\n",
       "is     5643.0   5102.0\n",
       "it     4445.0   4462.0\n",
       "of     7758.0   6586.0\n",
       "that   3425.0      NaN\n",
       "the   16801.0  15746.0\n",
       "this      NaN   3918.0\n",
       "to     6501.0   6937.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top(data, n=10, split_pos=len(neg_df)):\n",
    "    top_df = pd.concat([data.iloc[:split_pos].sum().T.nlargest(n),\n",
    "               data.iloc[split_pos:].sum().T.nlargest(n)],\n",
    "              axis=1)\n",
    "    top_df.columns = [True, False]\n",
    "    return(top_df)\n",
    "get_top(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there's a lot of pretty irrelevant words in the top here.  It's hard to really say anything about this.  Can you think of a way to get to more informative terms (i.e. ones that might give you some insight as to what words are positive versus negative?)\n",
    "\n",
    "Hint: Think about which tokens might be less informative.  Is there a way we learned to remove those?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_stop = all_df.iloc[:, ~all_df.columns.isin(ENGLISH_STOP_WORDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['acting', 224.0, 381.0],\n",
       "       ['action', 152.0, 157.0],\n",
       "       ['actors', 213.0, 272.0],\n",
       "       ['actually', 160.0, 256.0],\n",
       "       ['awful', nan, 147.0],\n",
       "       ['bad', 191.0, 671.0],\n",
       "       ['beautiful', 152.0, nan],\n",
       "       ['best', 435.0, 192.0],\n",
       "       ['better', 217.0, 302.0],\n",
       "       ['big', 175.0, 166.0],\n",
       "       ['bit', 163.0, nan],\n",
       "       ['boring', nan, 151.0],\n",
       "       ['br', 395.0, 421.0],\n",
       "       ['ca', 166.0, 194.0],\n",
       "       ['cast', 213.0, 179.0],\n",
       "       ['character', 366.0, 372.0],\n",
       "       ['characters', 366.0, 369.0],\n",
       "       ['come', nan, 155.0],\n",
       "       ['comedy', 158.0, 158.0],\n",
       "       ['course', 150.0, nan],\n",
       "       ['did', 413.0, 647.0],\n",
       "       ['different', 142.0, nan],\n",
       "       ['director', 239.0, 213.0],\n",
       "       ['does', 490.0, 539.0],\n",
       "       ['dvd', 182.0, nan],\n",
       "       ['end', 243.0, 273.0],\n",
       "       ['especially', 162.0, nan],\n",
       "       ['excellent', 180.0, nan],\n",
       "       ['fact', 160.0, 184.0],\n",
       "       ['family', 187.0, nan],\n",
       "       ['far', nan, 173.0],\n",
       "       ['feel', 167.0, nan],\n",
       "       ['film', 2077.0, 1842.0],\n",
       "       ['films', 371.0, 317.0],\n",
       "       ['fun', 149.0, nan],\n",
       "       ['funny', 203.0, 224.0],\n",
       "       ['gets', nan, 166.0],\n",
       "       ['going', 177.0, 232.0],\n",
       "       ['good', 771.0, 672.0],\n",
       "       ['got', nan, 175.0],\n",
       "       ['great', 640.0, 244.0],\n",
       "       ['guy', nan, 182.0],\n",
       "       ['half', nan, 163.0],\n",
       "       ['horror', 143.0, 178.0],\n",
       "       ['interesting', nan, 184.0],\n",
       "       ['job', 148.0, nan],\n",
       "       ['just', 699.0, 998.0],\n",
       "       ['know', 291.0, 352.0],\n",
       "       ['life', 375.0, 239.0],\n",
       "       ['like', 863.0, 1113.0],\n",
       "       ['little', 286.0, 268.0],\n",
       "       ['long', 168.0, 191.0],\n",
       "       ['look', 203.0, 201.0],\n",
       "       ['looking', nan, 150.0],\n",
       "       ['lot', 214.0, 187.0],\n",
       "       ['love', 411.0, 221.0],\n",
       "       ['make', 323.0, 469.0],\n",
       "       ['makes', 242.0, 171.0],\n",
       "       ['making', nan, 155.0],\n",
       "       ['man', 287.0, 245.0],\n",
       "       ['minutes', nan, 187.0],\n",
       "       ['money', nan, 162.0],\n",
       "       ['movie', 1852.0, 2349.0],\n",
       "       ['movies', 334.0, 381.0],\n",
       "       ['music', 237.0, nan],\n",
       "       ['new', 209.0, 161.0],\n",
       "       ['old', 199.0, 178.0],\n",
       "       ['original', nan, 156.0],\n",
       "       ['people', 380.0, 461.0],\n",
       "       ['perfect', 142.0, nan],\n",
       "       ['performance', 167.0, nan],\n",
       "       ['played', 187.0, nan],\n",
       "       ['plays', 149.0, nan],\n",
       "       ['plot', 233.0, 378.0],\n",
       "       ['point', nan, 150.0],\n",
       "       ['poor', nan, 151.0],\n",
       "       ['pretty', 143.0, 184.0],\n",
       "       ['probably', nan, 151.0],\n",
       "       ['quite', 240.0, nan],\n",
       "       ['real', 252.0, 198.0],\n",
       "       ['really', 518.0, 641.0],\n",
       "       ['reason', nan, 146.0],\n",
       "       ['right', 186.0, 165.0],\n",
       "       ['role', 181.0, nan],\n",
       "       ['saw', 171.0, 182.0],\n",
       "       ['say', 204.0, 262.0],\n",
       "       ['scene', 252.0, 278.0],\n",
       "       ['scenes', 281.0, 306.0],\n",
       "       ['script', nan, 196.0],\n",
       "       ['seen', 305.0, 317.0],\n",
       "       ['series', 189.0, nan],\n",
       "       ['shows', 142.0, nan],\n",
       "       ['story', 608.0, 502.0],\n",
       "       ['terrible', nan, 157.0],\n",
       "       ['thing', 149.0, 279.0],\n",
       "       ['things', 163.0, 189.0],\n",
       "       ['think', 371.0, 333.0],\n",
       "       ['thought', 185.0, 204.0],\n",
       "       ['time', 587.0, 577.0],\n",
       "       ['times', 168.0, 146.0],\n",
       "       ['trying', nan, 159.0],\n",
       "       ['want', 155.0, 202.0],\n",
       "       ['war', 168.0, nan],\n",
       "       ['watch', 332.0, 379.0],\n",
       "       ['watching', 180.0, 280.0],\n",
       "       ['way', 387.0, 359.0],\n",
       "       ['wonderful', 149.0, nan],\n",
       "       ['work', 200.0, 193.0],\n",
       "       ['world', 223.0, 151.0],\n",
       "       ['worst', nan, 219.0],\n",
       "       ['years', 248.0, 171.0],\n",
       "       ['young', 224.0, 151.0]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top(all_df.iloc[:, ~all_df.columns.isin(ENGLISH_STOP_WORDS)], n=90).reset_index().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how often the top words from negative appear in the positive reviews and vice versa.  Do these seem like good candidates for determining whether a review is positive or negative? If not, maybe expand to the top 10, or more.  The idea here is to get a list of terms that are pretty distinct between the two sets.\n",
    "\n",
    "One possible way to test is to use [log-likelihood ratio](https://wordhoard.northwestern.edu/userman/analysis-comparewords.html) as we discussed in class. In class we looked at texts with/without mentions of \"hot dog\".  What is our comparison text in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(analysis, reference, word):\n",
    "    # count of word in source\n",
    "    a = analysis[word].sum()\n",
    "    # count of word in reference\n",
    "    b = reference[word].sum()\n",
    "    # count of all words in source\n",
    "    c = analysis.sum().sum()\n",
    "    # count of all words in reference\n",
    "    d = reference.sum().sum()\n",
    "    print('counts analysis:', a)\n",
    "    print('counts reference:', b)\n",
    "    e1 = c*(a+b)/(c+d)\n",
    "    e2 = d*(a+b)/(c+d)\n",
    "    g = 2*((a*log(a/e1)) + (b*log(b/e2)))\n",
    "    print('G2: ', g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "counts analysis: 632\n",
      "counts reference: 811\n",
      "G2:  11.483395558996321\n",
      "character\n",
      "counts analysis: 364\n",
      "counts reference: 374\n",
      "G2:  0.34130117788180137\n",
      "story\n",
      "counts analysis: 485\n",
      "counts reference: 625\n",
      "G2:  9.25192431265053\n",
      "acting\n",
      "counts analysis: 369\n",
      "counts reference: 236\n",
      "G2:  39.54794928205044\n",
      "bad\n",
      "counts analysis: 668\n",
      "counts reference: 194\n",
      "G2:  309.98478292831265\n",
      "great\n",
      "counts analysis: 230\n",
      "counts reference: 654\n",
      "G2:  183.3398162287254\n"
     ]
    }
   ],
   "source": [
    "# the above gives us some candidates\n",
    "# function to do likelihood ratio test\n",
    "words_to_try = ['good', 'character', 'story', 'acting', 'bad', 'great']\n",
    "for w in words_to_try:\n",
    "    print(w)\n",
    "    log_likelihood(neg_df, pos_df, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary-based sentiment analysis \n",
    "Construct a list of the keywords you've found are good determinants if a review is positive or negative.  Use this list to \"score\" a review based on the number of times that word appears in the review.\n",
    "\n",
    "(Optional) A quick and fancy way of doing this is to use CountVectorizer's vocabulary parameter.  Think how you might be able to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function simple_tokenizer at 0x15e78d560>,\n",
       "                vocabulary=['good', 'great', 'best', 'love', 'story', 'bad',\n",
       "                            'worst', 'acting', 'poor'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_vocab = ['good', 'great', 'best', 'love', 'story']\n",
    "neg_vocab = ['bad', 'worst', 'acting', 'poor']\n",
    "sentiment_cv = CountVectorizer(tokenizer=simple_tokenizer, vocabulary=pos_vocab+neg_vocab)\n",
    "sentiment_cv.fit(neg+pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you do? How often do the negative reviews have a higher negative score than a positive score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of negative reviews with higher neg score: 0.28629359286293593\n",
      "% of positive reviews with higher pos score: 0.7480252764612955\n"
     ]
    }
   ],
   "source": [
    "# average score\n",
    "neg_sentiment_df = pd.DataFrame(sentiment_cv.transform(neg).toarray(),\n",
    "                      columns=sentiment_cv.get_feature_names())\n",
    "pos_sentiment_df = pd.DataFrame(sentiment_cv.transform(pos).toarray(),\n",
    "                      columns=sentiment_cv.get_feature_names())\n",
    "print('% of negative reviews with higher neg score:', \n",
    "      mean(neg_sentiment_df[neg_vocab].sum(axis=1)>neg_sentiment_df[pos_vocab].sum(axis=1)))\n",
    "print('% of positive reviews with higher pos score:', \n",
    "      mean(pos_sentiment_df[pos_vocab].sum(axis=1)>pos_sentiment_df[neg_vocab].sum(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based sentiment analysis\n",
    "Above we did some tinkering with our scoring and found it works to some extent, but it's likely not going to work the same on another dataset.  That is, it's not particularly generalizable.  However, modern sentiment analysis has moved away from dictionary-based scoring towards having sentiment be a \"classification\" problem.  \n",
    "\n",
    "For this last section, take a look at the transformers [Pipelines](https://github.com/huggingface/transformers#quick-tour-of-pipelines) functionality.  You'll see that with a few lines of code you can bring in an advanced sentiment analysis model.  Run this against the positive/negative corpus and see how it works compared to your work above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ecf970f9fb493599a7ce0e07fe73f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this might take a bit, these models aren't light-weight\n",
    "neg_parsed = [nlp(d) for d in neg]\n",
    "pos_parsed = [nlp(d) for d in pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using % labelled negative vs positive\n",
    "print('% neg reviews labelled negative:', \n",
    "      mean([doc[0]['label']=='NEGATIVE' for doc in neg_parsed]))\n",
    "print('% pos reviews labelled positive:', \n",
    "      mean([doc[0]['label']=='POSITIVE' for doc in pos_parsed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
