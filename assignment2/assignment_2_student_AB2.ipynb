{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Predicting sentiment\n",
    "In this assignment, you will be using the same sentiment analysis dataset as for Assignment 1, but you'll be looking to actually predict sentiment based on a variety of text-derived features.\n",
    "\n",
    "This dataset comes from [Mass et. al. (2011)](https://www.aclweb.org/anthology/P11-1015.pdf) and the full version is available [here](http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from collections import Counter\n",
    "import re\n",
    "from numpy import log, mean\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'pandas', 'transformers==2.4.1'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "I've saved a subset of the data in the data directory on the repository.  It is available as a pickled dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neg', 1233), ('pos', 1266)]\n",
      "[1233 1266]\n"
     ]
    }
   ],
   "source": [
    "# you will need to change this to where ever the file is stored\n",
    "data_location = './data/assignment_1_reviews.pkl'\n",
    "with open(data_location, 'rb') as f:\n",
    "    all_text = pickle.load(f)\n",
    "# corpora size\n",
    "print([(k, len(all_text[k])) for k in all_text])\n",
    "neg, pos = all_text.values()\n",
    "# for this assignment, let's combine all our data, but maintain the labels\n",
    "all_text = neg+pos\n",
    "# array makes for easier indexing\n",
    "is_positive = np.array([False]*len(neg)+[True]*len(pos))\n",
    "# check that they're equivalent\n",
    "print(np.bincount(is_positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating document feature vectors\n",
    "In this section, process all of your text data in order to create the following document-level feature vectors:\n",
    "\n",
    "- Word Counts (using `CountVectorizer`)\n",
    "- TF-IDF vectors (using `TfidfVectorizer`)\n",
    "- Non-Negative Matrix Factorization-based representations (using `NMF`)\n",
    "- Latent Dirichlet Allocation-based representations (using `LatentDirichletAllocation`)\n",
    "\n",
    "All of the design elements are up to you (e.g. tokenization, vocabulary limits, number of components).  It may make sense to try out a few different designs.  In the next section we'll do some evaluation of our different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "en = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(doc, model=en):\n",
    "    '''Tokenizer based on example from from week_1_intro notebook.\n",
    "    Filters non-alpha, url-like, and stopwords then lemmatizes each parsed token.'''\n",
    "    parsed = model(doc)\n",
    "    # Return list of lowercase parsed tokens that are alphanumeric and not urls \n",
    "    return([t.lemma_ for t in parsed if (t.is_alpha) and (not t.like_url) and (not t.is_stop)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-PRON-': 3,\n",
       " 'aa': 1,\n",
       " 'aaaaaaahhhhhhggg': 1,\n",
       " 'aaaggghhhhhhh': 1,\n",
       " 'aaah': 1,\n",
       " 'aaam': 1,\n",
       " 'aaja': 1,\n",
       " 'aalcc': 1,\n",
       " 'aaliyah': 4,\n",
       " 'aaran': 1,\n",
       " 'aardman': 2,\n",
       " 'aaron': 7,\n",
       " 'aawip': 2,\n",
       " 'aback': 1,\n",
       " 'abandon': 2,\n",
       " 'abandoned': 17,\n",
       " 'abandons': 1,\n",
       " 'abba': 2,\n",
       " 'abbey': 6,\n",
       " 'abbie': 3,\n",
       " 'abbot': 2,\n",
       " 'abbott': 4,\n",
       " 'abc': 8,\n",
       " 'abdominal': 1,\n",
       " 'abduction': 2,\n",
       " 'abductions': 1,\n",
       " 'abdul': 1,\n",
       " 'abdulrahman': 1,\n",
       " 'abe': 1,\n",
       " 'abel': 5,\n",
       " 'aberdeen': 1,\n",
       " 'abernethie': 1,\n",
       " 'aberration': 1,\n",
       " 'aberrations': 1,\n",
       " 'abhishek': 6,\n",
       " 'abiding': 1,\n",
       " 'abilities': 11,\n",
       " 'ability': 40,\n",
       " 'abit': 1,\n",
       " 'able': 152,\n",
       " 'ably': 4,\n",
       " 'abnormal': 2,\n",
       " 'aboard': 8,\n",
       " 'abolished': 2,\n",
       " 'abominable': 2,\n",
       " 'abominably': 1,\n",
       " 'abomination': 7,\n",
       " 'aboriginal': 1,\n",
       " 'aboriginee': 1,\n",
       " 'aborted': 1,\n",
       " 'abortion': 3,\n",
       " 'abound': 6,\n",
       " 'abounding': 1,\n",
       " 'abounds': 1,\n",
       " 'abraham': 3,\n",
       " 'abrasive': 1,\n",
       " 'abrasiveness': 1,\n",
       " 'abre': 1,\n",
       " 'abril': 2,\n",
       " 'abroad': 7,\n",
       " 'abrogated': 1,\n",
       " 'abrupt': 9,\n",
       " 'abruptly': 8,\n",
       " 'absence': 11,\n",
       " 'absent': 8,\n",
       " 'absentee': 2,\n",
       " 'absinthe': 1,\n",
       " 'absolute': 44,\n",
       " 'absolutely': 153,\n",
       " 'absolve': 1,\n",
       " 'absolves': 1,\n",
       " 'absorb': 3,\n",
       " 'absorbed': 6,\n",
       " 'absorbing': 1,\n",
       " 'absorption': 1,\n",
       " 'abstract': 10,\n",
       " 'abstracted': 1,\n",
       " 'abstractions': 1,\n",
       " 'abstracts': 1,\n",
       " 'absurd': 23,\n",
       " 'absurdest': 1,\n",
       " 'absurdist': 2,\n",
       " 'absurdities': 2,\n",
       " 'absurdity': 5,\n",
       " 'absurdly': 1,\n",
       " 'abundance': 3,\n",
       " 'abuse': 11,\n",
       " 'abused': 4,\n",
       " 'abuser': 2,\n",
       " 'abuses': 2,\n",
       " 'abusing': 3,\n",
       " 'abusive': 7,\n",
       " 'abuttment': 1,\n",
       " 'abwehr': 1,\n",
       " 'abysmal': 10,\n",
       " 'abysmally': 1,\n",
       " 'abyss': 1,\n",
       " 'ac': 1,\n",
       " 'academic': 3,\n",
       " 'academies': 1,\n",
       " 'academy': 30,\n",
       " 'acadiana': 1,\n",
       " 'accelerate': 1,\n",
       " 'accelerated': 3,\n",
       " 'accelerates': 1,\n",
       " 'accent': 48,\n",
       " 'accented': 3,\n",
       " 'accents': 20,\n",
       " 'accentuate': 3,\n",
       " 'accentuated': 1,\n",
       " 'accentuates': 1,\n",
       " 'accept': 29,\n",
       " 'acceptable': 18,\n",
       " 'acceptance': 10,\n",
       " 'accepted': 14,\n",
       " 'accepting': 6,\n",
       " 'accepts': 5,\n",
       " 'access': 3,\n",
       " 'accessabilty': 1,\n",
       " 'accessible': 6,\n",
       " 'accessing': 1,\n",
       " 'accessories': 1,\n",
       " 'accident': 26,\n",
       " 'accidental': 1,\n",
       " 'accidentally': 21,\n",
       " 'accidents': 1,\n",
       " 'acclaim': 2,\n",
       " 'acclaimed': 7,\n",
       " 'accolade': 1,\n",
       " 'accolades': 3,\n",
       " 'accommodate': 1,\n",
       " 'accomodations': 1,\n",
       " 'accompanied': 11,\n",
       " 'accompanies': 3,\n",
       " 'accompaniment': 3,\n",
       " 'accompanist': 1,\n",
       " 'accompany': 4,\n",
       " 'accompanying': 5,\n",
       " 'accomplice': 1,\n",
       " 'accomplish': 3,\n",
       " 'accomplished': 7,\n",
       " 'accomplishes': 2,\n",
       " 'accomplishment': 5,\n",
       " 'accomplishments': 2,\n",
       " 'accordance': 2,\n",
       " 'according': 21,\n",
       " 'accordingly': 2,\n",
       " 'accordion': 1,\n",
       " 'account': 14,\n",
       " 'accountability': 1,\n",
       " 'accountable': 1,\n",
       " 'accountant': 2,\n",
       " 'accounted': 1,\n",
       " 'accounting': 1,\n",
       " 'accounts': 8,\n",
       " 'accross': 1,\n",
       " 'accumulate': 1,\n",
       " 'accumulated': 1,\n",
       " 'accuracies': 1,\n",
       " 'accuracy': 8,\n",
       " 'accurate': 19,\n",
       " 'accurately': 6,\n",
       " 'accusation': 1,\n",
       " 'accusations': 3,\n",
       " 'accuse': 3,\n",
       " 'accused': 12,\n",
       " 'accuses': 3,\n",
       " 'accusing': 1,\n",
       " 'accustomed': 4,\n",
       " 'ace': 5,\n",
       " 'achieve': 14,\n",
       " 'achieved': 19,\n",
       " 'achievement': 10,\n",
       " 'achievements': 2,\n",
       " 'achieves': 9,\n",
       " 'achieving': 2,\n",
       " 'achille': 1,\n",
       " 'achilles': 1,\n",
       " 'aching': 3,\n",
       " 'achingly': 2,\n",
       " 'achoo': 1,\n",
       " 'acid': 9,\n",
       " 'acknowledge': 4,\n",
       " 'acknowledged': 2,\n",
       " 'acknowledgement': 1,\n",
       " 'acknowledgment': 1,\n",
       " 'ackroyd': 1,\n",
       " 'acquaintance': 3,\n",
       " 'acquainted': 3,\n",
       " 'acquire': 1,\n",
       " 'acquired': 5,\n",
       " 'acquiring': 2,\n",
       " 'acquit': 1,\n",
       " 'acquitted': 1,\n",
       " 'acreage': 1,\n",
       " 'acres': 1,\n",
       " 'acrobatic': 1,\n",
       " 'acrobatics': 2,\n",
       " 'act': 120,\n",
       " 'acted': 73,\n",
       " 'acting': 605,\n",
       " 'action': 309,\n",
       " 'actioner': 2,\n",
       " 'actioners': 1,\n",
       " 'actions': 45,\n",
       " 'actionscenes': 1,\n",
       " 'activate': 2,\n",
       " 'active': 6,\n",
       " 'actively': 4,\n",
       " 'activism': 1,\n",
       " 'activists': 1,\n",
       " 'activities': 8,\n",
       " 'activity': 5,\n",
       " 'actor': 215,\n",
       " 'actorly': 1,\n",
       " 'actors': 485,\n",
       " 'actress': 97,\n",
       " 'actresses': 37,\n",
       " 'acts': 47,\n",
       " 'actual': 74,\n",
       " 'actuality': 2,\n",
       " 'actually': 416,\n",
       " 'acuff': 1,\n",
       " 'acuteness': 1,\n",
       " 'ad': 12,\n",
       " 'ada': 4,\n",
       " 'adage': 2,\n",
       " 'adah': 3,\n",
       " 'adam': 27,\n",
       " 'adamason': 1,\n",
       " 'adams': 7,\n",
       " 'adamson': 4,\n",
       " 'adapt': 3,\n",
       " 'adaptation': 41,\n",
       " 'adaptations': 2,\n",
       " 'adapted': 12,\n",
       " 'adapter': 2,\n",
       " 'adapting': 3,\n",
       " 'adaption': 7,\n",
       " 'adaptors': 1,\n",
       " 'adapts': 2,\n",
       " 'add': 77,\n",
       " 'addams': 2,\n",
       " 'added': 42,\n",
       " 'addict': 8,\n",
       " 'addicted': 9,\n",
       " 'addiction': 12,\n",
       " 'addictive': 1,\n",
       " 'addicts': 3,\n",
       " 'adding': 13,\n",
       " 'addition': 19,\n",
       " 'additional': 2,\n",
       " 'additionally': 2,\n",
       " 'addle': 1,\n",
       " 'addled': 1,\n",
       " 'address': 9,\n",
       " 'addressed': 3,\n",
       " 'addresses': 2,\n",
       " 'addressing': 1,\n",
       " 'adds': 48,\n",
       " 'addtionally': 1,\n",
       " 'adebesi': 1,\n",
       " 'adebisi': 1,\n",
       " 'adelaide': 2,\n",
       " 'adele': 3,\n",
       " 'adenoid': 1,\n",
       " 'adentro': 1,\n",
       " 'adept': 4,\n",
       " 'adequate': 10,\n",
       " 'adequately': 3,\n",
       " 'adgth': 1,\n",
       " 'adhd': 1,\n",
       " 'adhere': 2,\n",
       " 'adherence': 1,\n",
       " 'adhering': 1,\n",
       " 'adieu': 1,\n",
       " 'adjacent': 2,\n",
       " 'adjani': 4,\n",
       " 'adjective': 1,\n",
       " 'adjectives': 2,\n",
       " 'adjust': 6,\n",
       " 'adjusted': 1,\n",
       " 'adkins': 3,\n",
       " 'administered': 1,\n",
       " 'administration': 5,\n",
       " 'administrative': 1,\n",
       " 'administrator': 1,\n",
       " 'admirable': 7,\n",
       " 'admirably': 4,\n",
       " 'admiral': 3,\n",
       " 'admiralty': 2,\n",
       " 'admiration': 2,\n",
       " 'admire': 14,\n",
       " 'admired': 4,\n",
       " 'admirer': 4,\n",
       " 'admirers': 1,\n",
       " 'admires': 1,\n",
       " 'admiring': 4,\n",
       " 'admission': 5,\n",
       " 'admit': 62,\n",
       " 'admits': 3,\n",
       " 'admitted': 5,\n",
       " 'admittedly': 7,\n",
       " 'admitting': 2,\n",
       " 'ado': 1,\n",
       " 'adolescence': 2,\n",
       " 'adolescent': 9,\n",
       " 'adolf': 1,\n",
       " 'adopt': 4,\n",
       " 'adopted': 11,\n",
       " 'adoptions': 1,\n",
       " 'adoptive': 4,\n",
       " 'adopts': 2,\n",
       " 'adorable': 11,\n",
       " 'adorably': 1,\n",
       " 'adore': 5,\n",
       " 'adored': 3,\n",
       " 'adoree': 1,\n",
       " 'adores': 2,\n",
       " 'adoring': 2,\n",
       " 'adrenaline': 1,\n",
       " 'adrian': 7,\n",
       " 'adrienne': 7,\n",
       " 'adroitly': 1,\n",
       " 'ads': 7,\n",
       " 'adult': 57,\n",
       " 'adulterers': 2,\n",
       " 'adultery': 3,\n",
       " 'adulthood': 2,\n",
       " 'adults': 32,\n",
       " 'advance': 10,\n",
       " 'advanced': 12,\n",
       " 'advances': 5,\n",
       " 'advancing': 2,\n",
       " 'advantage': 13,\n",
       " 'advent': 3,\n",
       " 'adventure': 54,\n",
       " 'adventurer': 2,\n",
       " 'adventures': 15,\n",
       " 'adventurous': 4,\n",
       " 'adversaries': 1,\n",
       " 'adversary': 1,\n",
       " 'adversely': 1,\n",
       " 'adversity': 3,\n",
       " 'advert': 1,\n",
       " 'advertise': 2,\n",
       " 'advertised': 6,\n",
       " 'advertising': 5,\n",
       " 'advice': 33,\n",
       " 'advices': 1,\n",
       " 'advise': 10,\n",
       " 'advised': 4,\n",
       " 'adviser': 2,\n",
       " 'advises': 2,\n",
       " 'advisor': 1,\n",
       " 'advocate': 2,\n",
       " 'advocating': 1,\n",
       " 'aerial': 10,\n",
       " 'aerials': 2,\n",
       " 'aerobics': 1,\n",
       " 'aesthetic': 7,\n",
       " 'aesthetics': 3,\n",
       " 'afar': 2,\n",
       " 'affability': 1,\n",
       " 'affable': 1,\n",
       " 'affair': 36,\n",
       " 'affaire': 1,\n",
       " 'affairs': 4,\n",
       " 'affect': 10,\n",
       " 'affectation': 1,\n",
       " 'affectations': 1,\n",
       " 'affected': 7,\n",
       " 'affecting': 6,\n",
       " 'affection': 7,\n",
       " 'affectionate': 3,\n",
       " 'affectionately': 2,\n",
       " 'affections': 4,\n",
       " 'affects': 5,\n",
       " 'affiliates': 1,\n",
       " 'affinity': 1,\n",
       " 'affirmation': 2,\n",
       " 'affirming': 1,\n",
       " 'affleck': 7,\n",
       " 'afflicted': 1,\n",
       " 'afford': 8,\n",
       " 'afforded': 3,\n",
       " 'afganistan': 1,\n",
       " 'afghan': 2,\n",
       " 'afghanistan': 7,\n",
       " 'afghans': 2,\n",
       " 'afi': 1,\n",
       " 'aficionado': 2,\n",
       " 'aficionados': 3,\n",
       " 'afictionado': 1,\n",
       " 'afloat': 2,\n",
       " 'afoot': 1,\n",
       " 'aforementioned': 8,\n",
       " 'afoul': 1,\n",
       " 'afraid': 26,\n",
       " 'africa': 18,\n",
       " 'african': 24,\n",
       " 'afrikaaner': 2,\n",
       " 'afrikaans': 4,\n",
       " 'afro': 1,\n",
       " 'afterall': 1,\n",
       " 'afterdark': 2,\n",
       " 'afterlife': 2,\n",
       " 'aftermath': 3,\n",
       " 'afternoon': 15,\n",
       " 'afterschool': 2,\n",
       " 'afterthought': 3,\n",
       " 'afterward': 11,\n",
       " 'afterwords': 2,\n",
       " 'afther': 1,\n",
       " 'agar': 1,\n",
       " 'agatha': 3,\n",
       " 'age': 87,\n",
       " 'aged': 21,\n",
       " 'ageing': 3,\n",
       " 'agencies': 1,\n",
       " 'agency': 11,\n",
       " 'agenda': 9,\n",
       " 'agendas': 3,\n",
       " 'agent': 31,\n",
       " 'agents': 13,\n",
       " 'ager': 2,\n",
       " 'agers': 1,\n",
       " 'ages': 29,\n",
       " 'agey': 1,\n",
       " 'aggies': 1,\n",
       " 'aggrapina': 1,\n",
       " 'aggravate': 1,\n",
       " 'aggravated': 1,\n",
       " 'aggravating': 3,\n",
       " 'aggravation': 1,\n",
       " 'aggregate': 1,\n",
       " 'aggression': 3,\n",
       " 'aggressive': 8,\n",
       " 'aggressor': 1,\n",
       " 'aghast': 2,\n",
       " 'agile': 1,\n",
       " 'aging': 9,\n",
       " 'agit': 1,\n",
       " 'agitated': 1,\n",
       " 'agitation': 1,\n",
       " 'aglow': 1,\n",
       " 'agnes': 3,\n",
       " 'agnostic': 1,\n",
       " 'ago': 108,\n",
       " 'agonizing': 5,\n",
       " 'agonizingly': 1,\n",
       " 'agony': 8,\n",
       " 'agree': 57,\n",
       " 'agreed': 14,\n",
       " 'agreeing': 5,\n",
       " 'agreement': 12,\n",
       " 'agrees': 13,\n",
       " 'agren': 2,\n",
       " 'aground': 1,\n",
       " 'agutter': 1,\n",
       " 'ah': 17,\n",
       " 'aharon': 1,\n",
       " 'ahead': 36,\n",
       " 'ahem': 1,\n",
       " 'ahh': 1,\n",
       " 'ahna': 2,\n",
       " 'aht': 1,\n",
       " 'aid': 8,\n",
       " 'aidan': 1,\n",
       " 'aide': 1,\n",
       " 'aided': 5,\n",
       " 'aiden': 2,\n",
       " 'aids': 2,\n",
       " 'aikawa': 2,\n",
       " 'aikido': 1,\n",
       " 'ailments': 1,\n",
       " 'aim': 8,\n",
       " 'aimed': 7,\n",
       " 'aimee': 1,\n",
       " 'aiming': 5,\n",
       " 'aimless': 4,\n",
       " 'aimlessly': 2,\n",
       " 'aims': 2,\n",
       " 'air': 52,\n",
       " 'airbrushed': 1,\n",
       " 'aircraft': 5,\n",
       " 'aired': 15,\n",
       " 'aires': 1,\n",
       " 'airhead': 1,\n",
       " 'airing': 3,\n",
       " 'airplane': 7,\n",
       " 'airplanes': 5,\n",
       " 'airport': 12,\n",
       " 'airs': 1,\n",
       " 'airstation': 1,\n",
       " 'airstrip': 2,\n",
       " 'airwolf': 1,\n",
       " 'airy': 1,\n",
       " 'aishwarya': 4,\n",
       " 'aishwarys': 1,\n",
       " 'aisles': 1,\n",
       " 'aj': 1,\n",
       " 'ajar': 1,\n",
       " 'ajax': 1,\n",
       " 'aka': 20,\n",
       " 'akasha': 3,\n",
       " 'akhnaton': 9,\n",
       " 'akiko': 1,\n",
       " 'akimbo': 1,\n",
       " 'akin': 14,\n",
       " 'akira': 1,\n",
       " 'akkaya': 1,\n",
       " 'aksu': 2,\n",
       " 'al': 20,\n",
       " 'ala': 5,\n",
       " 'alabama': 2,\n",
       " 'alain': 5,\n",
       " 'alan': 28,\n",
       " 'alarm': 3,\n",
       " 'alarmed': 2,\n",
       " 'alarming': 5,\n",
       " 'alas': 12,\n",
       " 'alaska': 4,\n",
       " 'alastair': 1,\n",
       " 'alba': 1,\n",
       " 'albania': 1,\n",
       " 'albanian': 1,\n",
       " 'albatross': 1,\n",
       " 'albeit': 11,\n",
       " 'albert': 17,\n",
       " 'alberta': 2,\n",
       " 'albertan': 1,\n",
       " 'alberto': 1,\n",
       " 'albery': 1,\n",
       " 'albiet': 1,\n",
       " 'albright': 3,\n",
       " 'album': 5,\n",
       " 'albums': 1,\n",
       " 'alby': 1,\n",
       " 'alcatraz': 1,\n",
       " 'alchemy': 1,\n",
       " 'alcohol': 8,\n",
       " 'alcoholic': 6,\n",
       " 'alcoholism': 2,\n",
       " 'aldous': 1,\n",
       " 'alec': 7,\n",
       " 'alegria': 1,\n",
       " 'alejandro': 2,\n",
       " 'alert': 3,\n",
       " 'alerts': 1,\n",
       " 'alesandr': 1,\n",
       " 'alex': 28,\n",
       " 'alexander': 10,\n",
       " 'alexanders': 1,\n",
       " 'alexandra': 5,\n",
       " 'alexis': 2,\n",
       " 'alf': 1,\n",
       " 'alfe': 1,\n",
       " 'alfonso': 2,\n",
       " 'alfre': 1,\n",
       " 'alfred': 16,\n",
       " 'algarve': 1,\n",
       " 'alger': 1,\n",
       " 'algren': 1,\n",
       " 'algy': 1,\n",
       " 'ali': 15,\n",
       " 'alias': 1,\n",
       " 'aliases': 2,\n",
       " 'alibi': 5,\n",
       " 'alic': 1,\n",
       " 'alice': 44,\n",
       " 'alien': 53,\n",
       " 'alienate': 2,\n",
       " 'alienated': 4,\n",
       " 'alienates': 1,\n",
       " 'alienating': 3,\n",
       " 'alienation': 3,\n",
       " 'aliens': 26,\n",
       " 'alignment': 1,\n",
       " 'alike': 18,\n",
       " 'alistair': 2,\n",
       " 'alive': 62,\n",
       " 'allan': 2,\n",
       " 'allayed': 1,\n",
       " 'allegations': 1,\n",
       " 'alleged': 3,\n",
       " 'allegedly': 2,\n",
       " 'allegiance': 1,\n",
       " 'allegorical': 3,\n",
       " 'allegories': 1,\n",
       " 'allegory': 5,\n",
       " 'allegra': 4,\n",
       " 'allen': 21,\n",
       " 'allergic': 2,\n",
       " 'alleviate': 1,\n",
       " 'alley': 6,\n",
       " 'alliance': 3,\n",
       " 'allied': 1,\n",
       " 'allies': 2,\n",
       " 'alligator': 1,\n",
       " 'allison': 2,\n",
       " 'allo': 4,\n",
       " 'allocated': 2,\n",
       " 'alloted': 1,\n",
       " 'allotted': 1,\n",
       " 'allow': 23,\n",
       " 'allowance': 1,\n",
       " 'allowances': 1,\n",
       " 'allowed': 41,\n",
       " 'allowing': 12,\n",
       " 'allows': 20,\n",
       " 'allstate': 1,\n",
       " 'alluded': 1,\n",
       " 'alludes': 3,\n",
       " 'alluding': 1,\n",
       " 'allure': 1,\n",
       " 'alluring': 4,\n",
       " 'allusion': 1,\n",
       " 'allusions': 2,\n",
       " 'allwyn': 1,\n",
       " 'ally': 2,\n",
       " 'allyson': 3,\n",
       " 'alma': 1,\n",
       " 'almasy': 6,\n",
       " 'almasys': 1,\n",
       " 'almighty': 2,\n",
       " 'almodóvar': 1,\n",
       " 'aloma': 1,\n",
       " 'alongside': 9,\n",
       " 'alonzo': 1,\n",
       " 'aloof': 2,\n",
       " 'aloofness': 1,\n",
       " 'alot': 1,\n",
       " 'aloud': 3,\n",
       " 'alpha': 2,\n",
       " 'alphabet': 1,\n",
       " 'alphaville': 1,\n",
       " 'alright': 24,\n",
       " 'altar': 2,\n",
       " 'alter': 6,\n",
       " 'alterations': 3,\n",
       " 'altercation': 1,\n",
       " 'altered': 3,\n",
       " 'altering': 2,\n",
       " 'alterio': 1,\n",
       " 'altermann': 1,\n",
       " 'alternate': 6,\n",
       " 'alternating': 1,\n",
       " 'alternatingly': 1,\n",
       " 'alternative': 9,\n",
       " 'alternatives': 1,\n",
       " 'alters': 1,\n",
       " 'altitude': 1,\n",
       " 'altman': 10,\n",
       " 'altogether': 11,\n",
       " 'altruistic': 3,\n",
       " 'aluminum': 1,\n",
       " 'alvin': 2,\n",
       " 'alvy': 3,\n",
       " 'alx': 1,\n",
       " 'alyssa': 3,\n",
       " 'alzheimer': 1,\n",
       " 'amadeus': 4,\n",
       " 'amalgam': 1,\n",
       " 'amanda': 10,\n",
       " 'amants': 1,\n",
       " 'amass': 1,\n",
       " 'amasses': 1,\n",
       " 'amateur': 21,\n",
       " 'amateurish': 21,\n",
       " 'amateurishly': 1,\n",
       " 'amateurs': 1,\n",
       " 'amato': 3,\n",
       " 'amature': 1,\n",
       " 'amazed': 7,\n",
       " 'amazement': 1,\n",
       " 'amazes': 5,\n",
       " 'amazing': 113,\n",
       " 'amazingly': 24,\n",
       " 'amazon': 11,\n",
       " 'amazons': 1,\n",
       " 'ambassador': 2,\n",
       " 'amber': 4,\n",
       " 'ambiance': 3,\n",
       " 'ambient': 1,\n",
       " 'ambiguity': 5,\n",
       " 'ambiguous': 8,\n",
       " 'ambition': 6,\n",
       " 'ambitions': 1,\n",
       " 'ambitious': 15,\n",
       " 'ambitiously': 1,\n",
       " 'ambivalence': 1,\n",
       " 'ambivalent': 2,\n",
       " 'ambivilance': 1,\n",
       " 'amble': 1,\n",
       " 'amblings': 1,\n",
       " 'ambulance': 3,\n",
       " 'ambulatory': 1,\n",
       " 'ambush': 3,\n",
       " 'ambushed': 1,\n",
       " 'ambushing': 1,\n",
       " 'ameche': 5,\n",
       " 'amelia': 2,\n",
       " 'amelie': 2,\n",
       " 'amended': 1,\n",
       " 'amendment': 1,\n",
       " 'amendments': 1,\n",
       " 'amends': 1,\n",
       " 'amenábar': 2,\n",
       " 'america': 76,\n",
       " 'american': 212,\n",
       " 'americana': 3,\n",
       " 'americanization': 1,\n",
       " 'americanized': 1,\n",
       " 'americans': 46,\n",
       " 'americas': 2,\n",
       " 'ames': 1,\n",
       " 'amiable': 1,\n",
       " 'amid': 5,\n",
       " 'amidst': 5,\n",
       " 'amiel': 1,\n",
       " 'amiens': 1,\n",
       " 'amigo': 1,\n",
       " 'amina': 3,\n",
       " 'amir': 6,\n",
       " 'amitabh': 3,\n",
       " 'amityville': 3,\n",
       " 'ammo': 2,\n",
       " 'ammunition': 1,\n",
       " 'amnesia': 4,\n",
       " 'amnesiac': 3,\n",
       " 'amok': 6,\n",
       " 'amor': 3,\n",
       " 'amoral': 3,\n",
       " 'amorous': 2,\n",
       " 'amounts': 11,\n",
       " 'ampas': 1,\n",
       " 'amphibious': 1,\n",
       " 'ample': 5,\n",
       " 'amplified': 2,\n",
       " 'amply': 1,\n",
       " 'amputee': 3,\n",
       " 'amruta': 3,\n",
       " 'amsterdam': 3,\n",
       " 'amudha': 2,\n",
       " 'amulet': 5,\n",
       " 'amuse': 2,\n",
       " 'amused': 6,\n",
       " 'amusement': 7,\n",
       " 'amusing': 37,\n",
       " 'amusingly': 1,\n",
       " 'amy': 12,\n",
       " 'amyotrophic': 1,\n",
       " 'anachronisms': 2,\n",
       " 'anachronistic': 2,\n",
       " 'anaconda': 2,\n",
       " 'anaemic': 1,\n",
       " 'anal': 3,\n",
       " 'analog': 1,\n",
       " 'analogous': 1,\n",
       " 'analogy': 2,\n",
       " 'analysis': 3,\n",
       " 'analyst': 1,\n",
       " 'analysts': 1,\n",
       " 'analyze': 4,\n",
       " 'analyzing': 2,\n",
       " 'ananka': 5,\n",
       " 'anarchist': 1,\n",
       " 'anastasia': 2,\n",
       " 'anatoly': 1,\n",
       " 'anatomically': 1,\n",
       " 'anatomy': 2,\n",
       " 'anchor': 3,\n",
       " 'anchorman': 1,\n",
       " 'anchors': 1,\n",
       " 'ancient': 28,\n",
       " 'ancillary': 1,\n",
       " 'ancken': 1,\n",
       " 'andalites': 1,\n",
       " 'andalou': 2,\n",
       " 'anderson': 28,\n",
       " 'andersson': 4,\n",
       " 'andes': 2,\n",
       " 'andie': 1,\n",
       " 'anding': 1,\n",
       " 'andoheb': 1,\n",
       " 'andre': 6,\n",
       " 'andrea': 3,\n",
       " 'andreas': 11,\n",
       " 'andrei': 3,\n",
       " 'andrew': 12,\n",
       " 'andrews': 26,\n",
       " 'android': 1,\n",
       " 'andy': 18,\n",
       " 'anecdote': 1,\n",
       " 'anecdotes': 2,\n",
       " 'anemic': 1,\n",
       " 'anesthesiologist': 1,\n",
       " 'anesthetic': 1,\n",
       " 'anew': 1,\n",
       " 'angel': 14,\n",
       " 'angela': 12,\n",
       " 'angeles': 20,\n",
       " 'angelic': 1,\n",
       " 'angelina': 2,\n",
       " 'angelis': 1,\n",
       " 'angella': 3,\n",
       " 'angelo': 2,\n",
       " 'angelopoulos': 2,\n",
       " 'angelos': 1,\n",
       " 'angels': 13,\n",
       " 'anger': 16,\n",
       " 'angered': 2,\n",
       " 'angers': 1,\n",
       " 'angharad': 1,\n",
       " 'angie': 3,\n",
       " 'angkatell': 1,\n",
       " 'angle': 15,\n",
       " 'angles': 27,\n",
       " 'anglicans': 1,\n",
       " 'angola': 1,\n",
       " 'angora': 1,\n",
       " 'angrier': 1,\n",
       " 'angrily': 1,\n",
       " 'angry': 35,\n",
       " 'angst': 4,\n",
       " 'angsts': 1,\n",
       " 'angsty': 1,\n",
       " 'anguish': 3,\n",
       " 'anguished': 3,\n",
       " 'anguishing': 1,\n",
       " 'angus': 1,\n",
       " 'anil': 3,\n",
       " 'animal': 25,\n",
       " 'animalism': 1,\n",
       " 'animalistic': 1,\n",
       " 'animals': 26,\n",
       " 'animaniacs': 1,\n",
       " 'animate': 1,\n",
       " 'animated': 34,\n",
       " 'animating': 1,\n",
       " 'animation': 53,\n",
       " 'animations': 3,\n",
       " 'animator': 2,\n",
       " 'animators': 8,\n",
       " 'animatronics': 1,\n",
       " 'anime': 8,\n",
       " 'animes': 1,\n",
       " 'animorphs': 2,\n",
       " 'aniston': 1,\n",
       " 'anita': 5,\n",
       " 'anjali': 3,\n",
       " 'anjelica': 2,\n",
       " 'ankh': 1,\n",
       " 'ankle': 1,\n",
       " 'ankrum': 1,\n",
       " 'ann': 23,\n",
       " 'anna': 17,\n",
       " 'annabelle': 3,\n",
       " 'annakin': 1,\n",
       " 'annals': 1,\n",
       " 'anne': 51,\n",
       " 'annette': 7,\n",
       " 'annie': 10,\n",
       " 'annihilation': 1,\n",
       " 'annis': 1,\n",
       " 'anniversary': 1,\n",
       " 'announce': 4,\n",
       " 'announced': 6,\n",
       " 'announcement': 4,\n",
       " 'announcer': 4,\n",
       " 'announcers': 1,\n",
       " 'announces': 2,\n",
       " 'announcing': 1,\n",
       " 'annoy': 3,\n",
       " 'annoyance': 3,\n",
       " 'annoyances': 1,\n",
       " 'annoyed': 18,\n",
       " 'annoying': 100,\n",
       " 'annoyingly': 3,\n",
       " 'annoys': 5,\n",
       " 'annual': 4,\n",
       " 'annulled': 1,\n",
       " 'annulment': 1,\n",
       " 'anodyne': 2,\n",
       " 'anomalies': 1,\n",
       " 'anonymity': 1,\n",
       " 'anonymous': 8,\n",
       " 'anorexia': 1,\n",
       " 'anorexic': 1,\n",
       " 'ansom': 4,\n",
       " 'answer': 37,\n",
       " 'answered': 4,\n",
       " 'answering': 1,\n",
       " 'answers': 14,\n",
       " 'antagonism': 1,\n",
       " 'antagonist': 5,\n",
       " 'antagonists': 1,\n",
       " 'antagonizing': 1,\n",
       " 'antara': 2,\n",
       " 'antarctica': 2,\n",
       " 'ante': 1,\n",
       " 'antena': 8,\n",
       " 'antenna': 1,\n",
       " 'anthem': 4,\n",
       " 'anthologies': 1,\n",
       " 'anthology': 2,\n",
       " 'anthony': 28,\n",
       " 'anthropological': 1,\n",
       " 'anthropologist': 2,\n",
       " 'anthropomorphic': 1,\n",
       " 'anti': 60,\n",
       " 'antichrist': 1,\n",
       " 'anticipated': 4,\n",
       " 'anticipation': 4,\n",
       " 'anticlimactic': 1,\n",
       " 'antics': 16,\n",
       " 'antidote': 6,\n",
       " 'antif': 1,\n",
       " 'antifreeze': 2,\n",
       " 'antipodean': 1,\n",
       " 'antipodes': 1,\n",
       " 'antiquated': 1,\n",
       " 'antisemitism': 2,\n",
       " 'antitank': 2,\n",
       " 'antitrust': 1,\n",
       " 'anton': 1,\n",
       " 'antonio': 8,\n",
       " 'antonioni': 1,\n",
       " 'antony': 5,\n",
       " 'antônio': 3,\n",
       " 'anupam': 2,\n",
       " 'anxieties': 1,\n",
       " 'anxiety': 5,\n",
       " 'anxious': 4,\n",
       " 'anxiously': 1,\n",
       " 'anybody': 17,\n",
       " 'anymore': 34,\n",
       " 'anyplace': 1,\n",
       " 'anytime': 4,\n",
       " 'anyways': 6,\n",
       " 'apache': 1,\n",
       " 'apalling': 1,\n",
       " 'aparichit': 1,\n",
       " 'aparna': 5,\n",
       " 'apart': 56,\n",
       " 'apartheid': 2,\n",
       " 'apartment': 22,\n",
       " 'apartments': 3,\n",
       " 'apathy': 1,\n",
       " 'ape': 9,\n",
       " 'apes': 6,\n",
       " 'aphrodite': 1,\n",
       " 'apiece': 1,\n",
       " 'aping': 1,\n",
       " 'aplenty': 1,\n",
       " 'aplomb': 1,\n",
       " 'apocalypse': 19,\n",
       " 'apocalyptic': 14,\n",
       " 'apolitical': 1,\n",
       " 'apollo': 1,\n",
       " 'apologies': 1,\n",
       " 'apologists': 1,\n",
       " 'apologize': 2,\n",
       " 'apologizes': 3,\n",
       " 'apologizing': 2,\n",
       " 'apology': 5,\n",
       " 'apostle': 1,\n",
       " 'appalled': 3,\n",
       " 'appalling': 16,\n",
       " 'appallingly': 2,\n",
       " 'apparantely': 1,\n",
       " 'apparent': 21,\n",
       " 'apparently': 66,\n",
       " 'apparitions': 1,\n",
       " 'appeal': 40,\n",
       " 'appealed': 1,\n",
       " 'appealing': 19,\n",
       " 'appeals': 4,\n",
       " 'appear': 59,\n",
       " 'appearance': 29,\n",
       " 'appearances': 15,\n",
       " 'appeared': 33,\n",
       " 'appearing': 17,\n",
       " 'appears': 89,\n",
       " 'appendicitis': 1,\n",
       " 'appetite': 5,\n",
       " 'appetizing': 1,\n",
       " 'applaud': 6,\n",
       " 'applauded': 3,\n",
       " 'applauding': 2,\n",
       " 'applause': 2,\n",
       " 'apple': 2,\n",
       " 'apples': 1,\n",
       " 'appliance': 2,\n",
       " 'applicable': 3,\n",
       " 'application': 1,\n",
       " 'applications': 1,\n",
       " 'applied': 8,\n",
       " 'applies': 6,\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word count for all text using CountVectorizer and tokenizer\n",
    "cv = CountVectorizer(tokenizer=tokenizer)\n",
    "count_vectors = cv.fit_transform(all_text).toarray()\n",
    "dict(zip(cv.get_feature_names(), count_vectors.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple tfidf vectorizer\n",
    "tfidf_v = TfidfVectorizer(tokenizer=tokenizer)\n",
    "# Get tfidf vectors of all text by using fit transform\n",
    "tfidf_vectors = tfidf_v.fit_transform(all_text).toarray()\n",
    "# Create dict from vecotrs\n",
    "all_text_tfidf_dict = dict(zip(tfidf_v.get_feature_names(), tfidf_vectors.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF\n",
    "nmf_n = 10\n",
    "nmf = NMF(n_components=nmf_n)\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "lda_n = 10\n",
    "lda = LatentDirichletAllocation(n_components=lda_n)\n",
    "lda_vecs = lda.fit_transform(count_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis on vectors\n",
    "It's important to do some initial exploration of the features you've engineered.  Remember the goal is to get some information out of text, so you want to ensure your features are informative.  In this case, informative would mean it gives some information about sentiment.\n",
    "\n",
    "Perform the following analysis and any additional checks that might be useful for creating a set of informative features:\n",
    "- Top words for positive versus negative (Counts and TF-IDF)\n",
    "- Topic model performance measures (NMF=Reconstruction error, LDA=Evidence Lower BOund (ELBO))\n",
    "- Average cosine similarity between negative review vecvtors and positive review vectors (for all vectors you've created)\n",
    "\n",
    "Tip: You can use the is_positive vector to subset your vectors.  You will likely need to have them in dense array format (use the `.toarray()` method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_words(corpus, cv=cv, num_words=10):\n",
    "    '''Gets the most frequent words in a corpus, using a count vectorizer on the generated corpus dict'''\n",
    "    corpus_dict = get_corpus_dict(corpus, cv)\n",
    "    return sorted(corpus_dict, key=corpus_dict.get, reverse=True)[:num_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_dict(corpus, cv=cv):\n",
    "    '''Creates a dictionary of the words and their counts in the corpus using a count vectorizer'''\n",
    "    v = cv.fit_transform(corpus).toarray()\n",
    "    corpus_dict = dict(zip(cv.get_feature_names(), v.sum(axis=0)))\n",
    "    return corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top words in negative reviews are: ['movie', 'film', 'like', 'bad', 'good', 'time', 'story', 'people', 'br', 'movies']\n",
      "The top words in positive reviews are: ['film', 'movie', 'like', 'good', 'great', 'story', 'time', 'best', 'love', 'br']\n"
     ]
    }
   ],
   "source": [
    "# Top 10 words - counts\n",
    "# Negative reviews\n",
    "neg_words_top = get_most_frequent_words(neg)\n",
    "print(f'The top words in negative reviews are: {neg_words_top}')\n",
    "\n",
    "# Positive reviews\n",
    "pos_words_top = get_most_frequent_words(pos)\n",
    "print(f'The top words in positive reviews are: {pos_words_top}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'film',\n",
       " 'good',\n",
       " 'like',\n",
       " 'great',\n",
       " 'story',\n",
       " 'time',\n",
       " 'love',\n",
       " 'best',\n",
       " 'think']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 words - TF-IDF\n",
    "# Positive\n",
    "pos_tfidf_dict = get_tfidf_dict(pos)\n",
    "sorted(pos_tfidf_dict, key=pos_tfidf_dict.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'film',\n",
       " 'like',\n",
       " 'bad',\n",
       " 'good',\n",
       " 'time',\n",
       " 'story',\n",
       " 'br',\n",
       " 'watch',\n",
       " 'movies']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 words - TF-IDF\n",
    "# Negative\n",
    "neg_tfidf_dict = get_tfidf_dict(neg)\n",
    "sorted(neg_tfidf_dict, key=neg_tfidf_dict.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the above results look? Ideally you should see that your features give some information that might help a model discern negative from positive reviews.  That means lower similarity inter-class and different words showing up as most frequent/relevant.  Experiment with your design choices on the steps above.  Your goal should be to get to a set of vectors that have lower inter-class similarity than intra-class similarity (e.g. positive reviews should be more similar to positive reviews than negative reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting sentiment\n",
    "As we did in week 2's notebook, we're now going to use these informative vectors to predict sentiment.  We'll be using `LinearSVC` in this exercise, but feel free to try out other models.\n",
    "\n",
    "Start by creating a train/test split for the dataset (typically 70%/30%).  We'll use the same split for all feature vectors for comparability. \n",
    "\n",
    "Do the following steps for all the feature vectors you developed above:\n",
    "- Start by creating a train/test split for the dataset (typically 70%/30%).  We'll use the same split for all feature vectors for comparability. \n",
    "- Train an SVM model on your feature vectors with the corresponding target values (positive/negative)\n",
    "- Test the SVM model on the test set and output the accuracy\n",
    "\n",
    "Tip: Sklearn has a train/test split functionality for generating train/test splits (`sklearn.model_selection.train_test_split`).  Since we want to use the same reviews, make sure you set a random_state (see the docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how you've designed your vectors, you may find that the topic models perform worse than the count vectors.  You may want to try a couple different configurations.  \n",
    "\n",
    "One key reason for this may be because if the goal is to use our test observations to simulate our \"new observations\", we haven't properly done that.  We've fit our vectorizers on the FULL corpus.  If our test observations are \"unseen\", that means our vectorizers should only be fit on the training corpus.\n",
    "\n",
    "Try this out: Split the unprocessed reviews, fit the vectorizer, then the model and then transform the test observations and predict.  See how the accuracy changes\n",
    "\n",
    "Tip: You may want to explore sklearn's `Pipelines`, which is designed for exactly this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
