{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Predicting sentiment\n",
    "In this assignment, you will be using the same sentiment analysis dataset as for Assignment 1, but you'll be looking to actually predict sentiment based on a variety of text-derived features.\n",
    "\n",
    "This dataset comes from [Mass et. al. (2011)](https://www.aclweb.org/anthology/P11-1015.pdf) and the full version is available [here](http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from collections import Counter\n",
    "import re\n",
    "from numpy import log, mean\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'pandas', 'transformers==2.4.1'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "I've saved a subset of the data in the data directory on the repository.  It is available as a pickled dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neg', 1233), ('pos', 1266)]\n",
      "[1233 1266]\n"
     ]
    }
   ],
   "source": [
    "# you will need to change this to where ever the file is stored\n",
    "data_location = './data/assignment_1_reviews.pkl'\n",
    "with open(data_location, 'rb') as f:\n",
    "    all_text = pickle.load(f)\n",
    "# corpora size\n",
    "print([(k, len(all_text[k])) for k in all_text])\n",
    "neg, pos = all_text.values()\n",
    "# for this assignment, let's combine all our data, but maintain the labels\n",
    "all_text = neg+pos\n",
    "first_pos = len(neg)\n",
    "# array makes for easier indexing\n",
    "is_positive = np.array([False]*len(neg)+[True]*len(pos))\n",
    "# check that they're equivalent\n",
    "print(np.bincount(is_positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating document feature vectors\n",
    "In this section, process all of your text data in order to create the following document-level feature vectors:\n",
    "\n",
    "- Word Counts (using `CountVectorizer`)\n",
    "- TF-IDF vectors (using `TfidfVectorizer`)\n",
    "- Non-Negative Matrix Factorization-based representations (using `NMF`)\n",
    "- Latent Dirichlet Allocation-based representations (using `LatentDirichletAllocation`)\n",
    "\n",
    "All of the design elements are up to you (e.g. tokenization, vocabulary limits, number of components).  It may make sense to try out a few different designs.  In the next section we'll do some evaluation of our different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and instantiate English model from Spacy\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "\n",
    "# Import and instantiaite trained model from Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(doc, model=en):\n",
    "    '''Tokenizer based on example from from week_1_intro notebook.\n",
    "    Filters non-alpha, url-like, and stopwords then lemmatizes each parsed token.'''\n",
    "    parsed = model(doc)\n",
    "    # Return list of lowercase parsed tokens that are alphanumeric and not urls \n",
    "    return([t.lemma_ for t in parsed if (t.is_alpha) and (not t.like_url) and (not t.is_stop)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get count vectors using fit transform on a count vectorizer\n",
    "def get_count_vectors(text, cv=cv):\n",
    "    return cv.fit_transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get count dictionary using count vectors and count vectorizer\n",
    "def get_count_dict(count_vectors, cv=cv):\n",
    "    return dict(zip(cv.get_feature_names(), count_vectors.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word counts and represent as dict\n",
    "count_vectors = get_count_vectors(all_text)\n",
    "count_dict = get_count_dict(count_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate simple tfidf vectorizer\n",
    "tfidf_v = TfidfVectorizer(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_vectors(text, tfidf_v=tfidf_v):\n",
    "    return tfidf_v.fit_transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF using CountVectorizer and tokenizer\n",
    "def get_tfidf_dict(tfidf_vectors, tfidf_v=tfidf_v):\n",
    "    return dict(zip(tfidf_v.get_feature_names(), tfidf_vectors.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tfidf vectors and represent as dict - each word is the key and the tfidf number is the value\n",
    "tfidf_vectors_all = get_tfidf_vectors(all_text)\n",
    "all_text_tfidf_dict = get_tfidf_dict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF using the tfidf vectors\n",
    "# Using 10 components - will explore different numbers later in the assignment\n",
    "nmf_n = 10\n",
    "nmf = NMF(n_components=nmf_n)\n",
    "nmf_vectors = nmf.fit_transform(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA using the count_vectors\n",
    "# Using 10 components - will explore different numbers later in the assignment\n",
    "lda_n = 10\n",
    "lda = LatentDirichletAllocation(n_components=lda_n)\n",
    "lda_vectors = lda.fit_transform(count_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis on vectors\n",
    "It's important to do some initial exploration of the features you've engineered.  Remember the goal is to get some information out of text, so you want to ensure your features are informative.  In this case, informative would mean it gives some information about sentiment.\n",
    "\n",
    "Perform the following analysis and any additional checks that might be useful for creating a set of informative features:\n",
    "- Top words for positive versus negative (Counts and TF-IDF)\n",
    "- Topic model performance measures (NMF=Reconstruction error, LDA=Evidence Lower BOund (ELBO))\n",
    "- Average cosine similarity between negative review vecvtors and positive review vectors (for all vectors you've created)\n",
    "\n",
    "Tip: You can use the is_positive vector to subset your vectors.  You will likely need to have them in dense array format (use the `.toarray()` method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_words(corpus, cv=cv, num_words=10):\n",
    "    '''Gets the most frequent words in a corpus, using a count vectorizer on the generated corpus dict'''\n",
    "    corpus_dict = get_corpus_dict(corpus, cv)\n",
    "    return sorted(corpus_dict, key=corpus_dict.get, reverse=True)[:num_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_dict(corpus, cv=cv):\n",
    "    '''Creates a dictionary of the words and their counts in the corpus using a count vectorizer'''\n",
    "    v = cv.fit_transform(corpus).toarray()\n",
    "    corpus_dict = dict(zip(cv.get_feature_names(), v.sum(axis=0)))\n",
    "    return corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top words by count in negative reviews are: ['movie', 'film', 'like', 'bad', 'good', 'time', 'story', 'people', 'br', 'movies']\n",
      "The top words by count in positive reviews are: ['film', 'movie', 'like', 'good', 'great', 'story', 'time', 'best', 'love', 'br']\n"
     ]
    }
   ],
   "source": [
    "# Top 10 words - counts\n",
    "# Negative reviews\n",
    "neg_words_top = get_most_frequent_words(neg)\n",
    "print(f'The top words by count in negative reviews are: {neg_words_top}')\n",
    "\n",
    "# Positive reviews\n",
    "pos_words_top = get_most_frequent_words(pos)\n",
    "print(f'The top words by count in positive reviews are: {pos_words_top}')\n",
    "\n",
    "# Note: these are not very informative! As we've seen so far in the course, word counts are not the best \n",
    "# metric for determining sentiment, as evidenced by the overlap between these two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tried using the trained nlp english mode, but there wasn't a significant improvement in performance.\n",
    "# However, theere was a significant decrease in efficiency, so I'm removing it from the tf-idf comptutation below.\n",
    "def nlp_tokenizer(doc, model=nlp):\n",
    "    '''Tokenizer based on example from from week_1_intro notebook.\n",
    "    Filters non-alpha, url-like, and stopwords then lemmatizes each parsed token.\n",
    "    Uses advanced trained nlp model from spacy by default.'''\n",
    "    parsed = model(doc)\n",
    "    # Return list of lowercase parsed tokens that are alphanumeric and not urls \n",
    "    return([t.lemma_ for t in parsed if (t.is_alpha) and (not t.like_url) and (not t.is_stop)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impose a min and max df to ensure the words aren't too frequent or infrequent\n",
    "tfidf_v2 = TfidfVectorizer(tokenizer=tokenizer, min_df=0.01, max_df=0.9)\n",
    "\n",
    "def get_top_tfidf_words(text, tfidf_v=tfidf_v2, num_words=10):\n",
    "    '''Gets the top words from tfidf vectors, based on tfidf values'''\n",
    "    tfidf_vectors = get_tfidf_vectors(text, tfidf_v=tfidf_v2)\n",
    "    tfidf_dict = dict(zip(tfidf_v.get_feature_names(), tfidf_vectors.sum(axis=0)))\n",
    "    return sorted(tfidf_dict, key=tfidf_dict.get, reverse=True)[:num_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top words by tfidf in positive reviews are: ['movie', 'film', 'good', 'like', 'great', 'story', 'time', 'best', 'love', 'watch']\n",
      "The top words by tfidf in negative reviews are: ['movie', 'film', 'like', 'bad', 'good', 'time', 'story', 'br', 'people', 'movies']\n"
     ]
    }
   ],
   "source": [
    "# Top 10 words - TF-IDF\n",
    "# Positive\n",
    "pos_top_tfidf_words = get_top_tfidf_words(pos)\n",
    "print(f'The top words by tfidf in positive reviews are: {pos_top_tfidf_words}')\n",
    "\n",
    "# Negative\n",
    "neg_top_tfidf_words = get_top_tfidf_words(neg)\n",
    "print(f'The top words by tfidf in negative reviews are: {neg_top_tfidf_words}')\n",
    "\n",
    "# Note: as you can see, these don't appear to be very helpful either! I'm thinking we will need a more \n",
    "# nuanced approach to understanding sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from week_2_vectors\n",
    "def display_components(model, word_features, top_display=5):\n",
    "    # utility for displaying respresentative words per component for topic models\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        top_words_idx = topic.argsort()[::-1][:top_display]\n",
    "        top_words = [word_features[i] for i in top_words_idx]\n",
    "        print(\" \".join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_nmf(text, n_components=10):\n",
    "    '''Helper function that initializes an nmf'''\n",
    "    # Set random state for reproducability\n",
    "    # Init with 'nndsvd' because according to the documentation it is better for sparseness\n",
    "    # Added alpha value to regularize, so calculation is more stable\n",
    "    nmf = NMF(n_components=n_components, alpha=0.1, random_state=101, init='nndsvd')\n",
    "    tfidf_vectors = get_tfidf_vectors(text, tfidf_v=tfidf_v2)\n",
    "    nmf_vectors = nmf.fit_transform(tfidf_vectors)\n",
    "    return nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reconstruction error for NMF with 30 components is: 46.08429797250744\n",
      "The reconstruction error for NMF with 50 components is: 45.027862802972145\n",
      "The reconstruction error for NMF with 75 components is: 43.891218393080685\n",
      "The reconstruction error for NMF with 100 components is: 42.85723091916044\n",
      "The reconstruction error for NMF with 150 components is: 40.95881272754923\n"
     ]
    }
   ],
   "source": [
    "# Compare reconstruction error for different number of components\n",
    "for n_components in [30,50,75,100,150]:\n",
    "    nmf = init_nmf(all_text, n_components)\n",
    "    # Top model performance metrics for nmf \n",
    "    print(f'The reconstruction error for NMF with {n_components} components is: {nmf.reconstruction_err_}')\n",
    "    \n",
    "# Note: this takes a long time to run, so I've saved the results: I tried 3,7,10,15 components, but\n",
    "# the error rate significantly decreased with 30+ components. I ultimately chose 75 because there was a slight elbow,\n",
    "# indicating that the gains by further increases were not necessarily worth the extra computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "film heard quality lot directed making hollywood makes previous festival\n",
      "Topic 1:\n",
      "movie kind theater horrible main makes sure amazing nudity b\n",
      "Topic 2:\n",
      "role plays actor played performance play job lead leading playing\n",
      "Topic 3:\n",
      "like look feel looks trying guys totally called cut feels\n",
      "Topic 4:\n",
      "br audience opera money okay line screenplay actor wood credit\n",
      "Topic 5:\n",
      "series episodes television final carry season usual doctor total release\n",
      "Topic 6:\n",
      "good pretty acting job nice makes thriller easy lot coming\n",
      "Topic 7:\n",
      "book read novel books completely adaptation changes version totally reading\n",
      "Topic 8:\n",
      "bad acting gave badly guys ridiculous low watching usually flick\n",
      "Topic 9:\n",
      "funny comedy laugh jokes comedies humor humour hilarious american comedic\n",
      "Topic 10:\n",
      "seen absolutely worse having times black far read maybe possible\n",
      "Topic 11:\n",
      "horror gore blood scary slasher creepy suspense genre scared flick\n",
      "Topic 12:\n",
      "great greatest acting ending brilliant wonderful job makes casting not\n",
      "Topic 13:\n",
      "movies watched comment enjoy entertaining naked mind imdb standards not\n",
      "Topic 14:\n",
      "monster creature giant footage jack goes stars reason doctor god\n",
      "Topic 15:\n",
      "love loved wonderful husband romantic falls beautiful girl world fall\n",
      "Topic 16:\n",
      "action van fight sequences martial arts western gun adventure exciting\n",
      "Topic 17:\n",
      "camera shot times sound looking screen long shots editing angles\n",
      "Topic 18:\n",
      "family true home coming class father brother wonderful town relationships\n",
      "Topic 19:\n",
      "story line stories told worth telling based short seeing amazing\n",
      "Topic 20:\n",
      "watch watching worth times hours chance sit laugh entertaining friends\n",
      "Topic 21:\n",
      "music musical city rock soundtrack band culture art songs amazing\n",
      "Topic 22:\n",
      "scene room left low dialog final away half script climax\n",
      "Topic 23:\n",
      "war world soldier charlie soldiers german american japanese hollywood ii\n",
      "Topic 24:\n",
      "effects special acting decent budget scary look american worse sound\n",
      "Topic 25:\n",
      "video store version rented available home day local release copy\n",
      "Topic 26:\n",
      "game games playing level bond play cool played greatest real\n",
      "Topic 27:\n",
      "school high day girl students gay history student team popular\n",
      "Topic 28:\n",
      "worst horrible acting watching script garbage redeeming writing come poor\n",
      "Topic 29:\n",
      "plot twists acting sub audience attempts surprising twist holes silly\n",
      "Topic 30:\n",
      "scenes sex couple looks instead big japanese acting feel okay\n",
      "Topic 31:\n",
      "people real etc kind reality hate kill government not ed\n",
      "Topic 32:\n",
      "character main world actress development male ryan annoying works picture\n",
      "Topic 33:\n",
      "end happy beginning ending highly edge expected left recommend ended\n",
      "Topic 34:\n",
      "films silent recent low certainly genre budget look type hollywood\n",
      "Topic 35:\n",
      "characters dialogue annoying main viewer half sequel care gay major\n",
      "Topic 36:\n",
      "best beautiful perfect award songs deserved cop academy favorite picture\n",
      "Topic 37:\n",
      "original version sequel remake hollywood island etc classic production hitchcock\n",
      "Topic 38:\n",
      "man police mr dead comes killer footage plays woman experience\n",
      "Topic 39:\n",
      "think shows romance especially maybe liked mind sure watched lines\n",
      "Topic 40:\n",
      "star rock fans band battle julie space wrong far need\n",
      "Topic 41:\n",
      "women men sex woman male violence female beautiful girls sexual\n",
      "Topic 42:\n",
      "thought found looked black recommend corny enjoy enjoyed watching rating\n",
      "Topic 43:\n",
      "cinema history piece city silent human world humour indian directors\n",
      "Topic 44:\n",
      "work getting writers piece level earlier makes money fails having\n",
      "Topic 45:\n",
      "kids let mom adults song kid home audience seeing heads\n",
      "Topic 46:\n",
      "dvd buy version released today live sound watched students available\n",
      "Topic 47:\n",
      "time waste long watching money spent worth simply screen big\n",
      "Topic 48:\n",
      "cast excellent fine performance performances production supporting crime dark entire\n",
      "Topic 49:\n",
      "director writer art talented lead actor far beautiful known american\n",
      "Topic 50:\n",
      "way find come right couple bond probably kind different science\n",
      "Topic 51:\n",
      "know lot friends let taken outside career actually saying joe\n",
      "Topic 52:\n",
      "bit fan big long ending liked look rare main moments\n",
      "Topic 53:\n",
      "saw remember loved liked seeing theater went stupid idea screen\n",
      "Topic 54:\n",
      "episode season episodes tv shows hope favorite south park television\n",
      "Topic 55:\n",
      "guy guys girl train hitchcock stupid home gay gets fight\n",
      "Topic 56:\n",
      "dr fi sci space earth budget alien low planet doctor\n",
      "Topic 57:\n",
      "mother father son daughter child wants law death relationship house\n",
      "Topic 58:\n",
      "better script tv definitely need rock try wrong deserve expected\n",
      "Topic 59:\n",
      "going island poor girl minutes completely hell boat team things\n",
      "Topic 60:\n",
      "want absolutely money away sit guess reason rent things viewing\n",
      "Topic 61:\n",
      "little found john things hard touch kid dead lot girl\n",
      "Topic 62:\n",
      "actors script performances production budget quality crazy follow lost forget\n",
      "Topic 63:\n",
      "new york police french city death needs ray drug chase\n",
      "Topic 64:\n",
      "david wife year visit weird brother george father parts badly\n",
      "Topic 65:\n",
      "thing right fact mean god actually absolutely earth type die\n",
      "Topic 66:\n",
      "boring interesting minutes gets manage cliché point subject find pointless\n",
      "Topic 67:\n",
      "old years year ago friends production classic street remember later\n",
      "Topic 68:\n",
      "got watched sex sorry right ok shows pretty kid night\n",
      "Topic 69:\n",
      "cartoon animation disney cat animated dog voice short silent ship\n",
      "Topic 70:\n",
      "life real gives brought lives convincing true sense human documentary\n",
      "Topic 71:\n",
      "fun perfect humor enjoyable moments plenty looking pretty wonderful entertaining\n",
      "Topic 72:\n",
      "terrible money acting awful lines worse stupid friends poorly hard\n",
      "Topic 73:\n",
      "children parents child tv days remember instead television adults problem\n",
      "Topic 74:\n",
      "young ryan robert jack taylor lee woman house early girl\n"
     ]
    }
   ],
   "source": [
    "nmf_n = 75\n",
    "nmf = NMF(n_components=nmf_n, random_state=101)\n",
    "tfidf_vectors_all = get_tfidf_vectors(all_text, tfidf_v=tfidf_v2)\n",
    "nmf_vectors_all = nmf.fit_transform(tfidf_vectors)\n",
    "\n",
    "# Print out the components and analyze them\n",
    "display_components(nmf, tfidf_v2.get_feature_names(), 10)\n",
    "\n",
    "# Note: for 10 components, they appear to be split along genres and/or types of movies/TV.\n",
    "# This split might be helpful, but it's still unclear exactly how this might be useful. Again, ultimately I landed on\n",
    "# 75 as the number of components, which is what I've included above. 75 components also appears to split along\n",
    "# similar lines, but also includes production type (e.g. low-quality, camera styles, etc.). Much more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new count vectorizer with min and max df\n",
    "cv2 = CountVectorizer(tokenizer=tokenizer, min_df=0.01, max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lda(text, n_components):\n",
    "    '''Helper function to initialize an lDA'''\n",
    "    # Increase the learning offset slightly to favor later reviews in the learning process\n",
    "    # I played aroun with learning decay, but didn't find anything better than the default rate of 0.7\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, learning_offset=15, random_state=101)\n",
    "    count_vectors = cv2.fit_transform(text).toarray()\n",
    "    lda_vectors = lda.fit_transform(count_vectors)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bound for LDA with 1 components is: 893.6286335855411\n",
      "The bound for LDA with 2 components is: 885.4677455027298\n",
      "The bound for LDA with 3 components is: 901.1347060471696\n",
      "The bound for LDA with 4 components is: 911.5578782461239\n",
      "The bound for LDA with 5 components is: 926.3874138501176\n",
      "The bound for LDA with 6 components is: 938.8797631098181\n",
      "The bound for LDA with 7 components is: 953.125259602031\n",
      "The bound for LDA with 8 components is: 968.8322869166778\n"
     ]
    }
   ],
   "source": [
    "# Compare lda bounds for different number of components\n",
    "for n_components in range(1,9):\n",
    "    lda = init_lda(all_text, n_components)\n",
    "    # Top model performance metrics for nmf \n",
    "    print(f'The bound for LDA with {n_components} components is: {lda.bound_}')\n",
    "    \n",
    "# Note: it appears that the bound value increases with the number of components. I tried 30, 50, 100, 150, 300, \n",
    "# and higher and the values kept getting worse. So I tried 1-15 and then realized I could use 1-8 to \n",
    "# tune hyperparameters and found that 2 was the best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "film story like br good films great life time man love character best characters way young world scenes director little\n",
      "Topic 1:\n",
      "movie like film good bad time people watch movies think acting know seen plot great watching way better story funny\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "lda_n = 2\n",
    "lda_all = LatentDirichletAllocation(n_components=lda_n, random_state=101)\n",
    "count_vectors_all = cv2.fit_transform(all_text).toarray()\n",
    "lda_vectors_all = lda_all.fit_transform(count_vectors_all)\n",
    "\n",
    "# Print out the components and analyze them\n",
    "display_components(lda_all, cv2.get_feature_names(), 20)\n",
    "\n",
    "# Note: while 2 components might have the lowest bound, it doesn't give much information when displayed with the\n",
    "# above function. \n",
    "# I also analyzed 10 components and 20 components. The split was more helpful in this case, but still didn't seem\n",
    "# very useful for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(COUNT) neg-to-neg: 0.12424753323103478 neg-to-pos: 0.11098543653407493 pos-to-pos: 0.11192883224959577\n",
      "(TF-IDF) neg-to-neg: 0.056494811837617544 neg-to-pos: 0.048939370583884215 pos-to-pos: 0.05228368790762318\n",
      "(NMF) neg-to-neg: 0.1938030869506251 neg-to-pos: 0.17348866712892863 pos-to-pos: 0.1797680408465853\n",
      "(LDA) neg-to-neg: 0.7827927985147735 neg-to-pos: 0.7131254067249214 pos-to-pos: 0.7583620305511937\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity - counts\n",
    "count_sims = cosine_similarity(count_vectors_all)\n",
    "\n",
    "# Cosine similarity - tfidf\n",
    "tfidf_sims = cosine_similarity(tfidf_vectors_all)\n",
    "\n",
    "# Cosine similarity - nmf\n",
    "nmf_sims = cosine_similarity(nmf_vectors_all)\n",
    "\n",
    "# Cosine similarity - lda\n",
    "lda_sims = cosine_similarity(lda_vectors_all)\n",
    "\n",
    "sims_data = {'(COUNT)': count_sims, '(TF-IDF)': tfidf_sims, '(NMF)': nmf_sims, '(LDA)': lda_sims}\n",
    "\n",
    "# compare positive to negative average distance\n",
    "# code from week_2_vectors\n",
    "for key, s_matrix in sims_data.items():\n",
    "    print(f'{key} neg-to-neg:', s_matrix[:first_pos, :first_pos].mean(axis=1).mean(),\n",
    "          'neg-to-pos:', s_matrix[:first_pos, first_pos:].mean(axis=1).mean(),\n",
    "          'pos-to-pos:', s_matrix[first_pos:, first_pos:].mean(axis=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the above results look? Ideally you should see that your features give some information that might help a model discern negative from positive reviews.  That means lower similarity inter-class and different words showing up as most frequent/relevant.  Experiment with your design choices on the steps above.  Your goal should be to get to a set of vectors that have lower inter-class similarity than intra-class similarity (e.g. positive reviews should be more similar to positive reviews than negative reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generally speaking for all of the comparison types, intra-class similarity is higher than inter-class, \n",
    "# but not by much for pos-to-pos. This is especially true for the count comparison. It seems like negatives are \n",
    "# more similar to each other than positives or positive-negatives. This might be explained by the type of review - \n",
    "# people are more likely to be emphatic and passionate about their negative reviews than their positive reviews. It\n",
    "# might be the case that this sentiment became encoded in the analysis. \n",
    "#Overall, itappears to be successful collection of calculations. \n",
    "\n",
    "# I included my notes above on various approaches I explored to optimize the different vector outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting sentiment\n",
    "As we did in week 2's notebook, we're now going to use these informative vectors to predict sentiment.  We'll be using `LinearSVC` in this exercise, but feel free to try out other models.\n",
    "\n",
    "Start by creating a train/test split for the dataset (typically 70%/30%).  We'll use the same split for all feature vectors for comparability. \n",
    "\n",
    "Do the following steps for all the feature vectors you developed above:\n",
    "- Start by creating a train/test split for the dataset (typically 70%/30%).  We'll use the same split for all feature vectors for comparability. \n",
    "- Train an SVM model on your feature vectors with the corresponding target values (positive/negative)\n",
    "- Test the SVM model on the test set and output the accuracy\n",
    "\n",
    "Tip: Sklearn has a train/test split functionality for generating train/test splits (`sklearn.model_selection.train_test_split`).  Since we want to use the same reviews, make sure you set a random_state (see the docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0]*len(neg)+[1]*len(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_text, y, test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how you've designed your vectors, you may find that the topic models perform worse than the count vectors.  You may want to try a couple different configurations.  \n",
    "\n",
    "One key reason for this may be because if the goal is to use our test observations to simulate our \"new observations\", we haven't properly done that.  We've fit our vectorizers on the FULL corpus.  If our test observations are \"unseen\", that means our vectorizers should only be fit on the training corpus.\n",
    "\n",
    "Try this out: Split the unprocessed reviews, fit the vectorizer, then the model and then transform the test observations and predict.  See how the accuracy changes\n",
    "\n",
    "Tip: You may want to explore sklearn's `Pipelines`, which is designed for exactly this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer_accuracy(vectorizer, X_train, X_test, y_train, y_test):\n",
    "    model = LinearSVC(random_state=101)\n",
    "    train_vectors = vectorizer.fit_transform(X_train).toarray()\n",
    "    model.fit(train_vectors, y_train)\n",
    "    test_vectors = vectorizer.transform(X_test).toarray()\n",
    "    test_preds = model.predict(test_vectors)\n",
    "    return accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for count is: 0.8133333333333334\n"
     ]
    }
   ],
   "source": [
    "svc_count = LinearSVC(random_state=101)\n",
    "count_accuracy = get_vectorizer_accuracy(cv, X_train, X_test, y_train, y_test)\n",
    "print(f'The accuracy for count is: {count_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for TF-IDF is: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Instantiate simple tfidf vectorizer\n",
    "tfidf_v3 = TfidfVectorizer(tokenizer=tokenizer)\n",
    "tfidf_accuracy = get_vectorizer_accuracy(tfidf_v3, X_train, X_test, y_train, y_test)\n",
    "print(f'The accuracy for TF-IDF is: {tfidf_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_v4 = TfidfVectorizer(tokenizer=tokenizer)\n",
    "\n",
    "def get_nmf_accuracy(X_train, X_test, y_train, y_test):\n",
    "    # Initialize svc model\n",
    "    model = LinearSVC(random_state=101)\n",
    "    # Initialize nmf\n",
    "    nmf = NMF(n_components=7, random_state=101)\n",
    "    # Get tfidf vectors for train data\n",
    "    tfidf_train_vectors = tfidf_v4.fit_transform(X_train).toarray()\n",
    "    # Use nmf to fit transform the tfidf vectors for train data\n",
    "    train_vectors = nmf.fit_transform(tfidf_train_vectors)\n",
    "    # Fit the svc model\n",
    "    model.fit(train_vectors, y_train)\n",
    "    # Use nmf to fit transform the tfidf vectors for test data\n",
    "    tfidf_test_vectors = tfidf_v4.fit_transform(X_test).toarray()\n",
    "    # Get the test vectors\n",
    "    test_vectors = nmf.fit_transform(tfidf_test_vectors)\n",
    "    # Make test predictions\n",
    "    test_preds = model.predict(test_vectors)\n",
    "    return accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for NMF is: 0.5293333333333333\n"
     ]
    }
   ],
   "source": [
    "nmf_accuracy = get_nmf_accuracy(X_train, X_test, y_train, y_test)\n",
    "print(f'The accuracy for NMF is: {nmf_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv4 = CountVectorizer(tokenizer=tokenizer)\n",
    "\n",
    "def get_lda_accuracy(X_train, X_test, y_train, y_test):\n",
    "    # Initialize svc model\n",
    "    model = LinearSVC(random_state=101)\n",
    "    # Initialize nmf\n",
    "    lda = LatentDirichletAllocation(n_components=98, learning_offset=15, random_state=101)\n",
    "    # Get count vectors for train data\n",
    "    count_train_vectors = cv4.fit_transform(X_train).toarray()\n",
    "    # Use nmf to fit transform the count vectors for train data\n",
    "    train_vectors = lda.fit_transform(count_train_vectors)\n",
    "    # Fit the svc model\n",
    "    model.fit(train_vectors, y_train)\n",
    "    # Use nmf to fit transform the tfidf vectors for test data\n",
    "    count_test_vectors = cv4.fit_transform(X_test).toarray()\n",
    "    # Get the test vectors\n",
    "    test_vectors = lda.fit_transform(count_test_vectors)\n",
    "    # Make test predictions\n",
    "    test_preds = model.predict(test_vectors)\n",
    "    return accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for LDA is: 0.5093333333333333\n"
     ]
    }
   ],
   "source": [
    "lda_accuracy = get_lda_accuracy(X_train, X_test, y_train, y_test)\n",
    "print(f'The accuracy for LDA is: {lda_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I found that updating the number of components for nmf and lda had significant impact on the accuracy.\n",
    "# But it was the inverse of what I found in previous sections: for NMF, accuracy decreased with an increase of \n",
    "# components; for LDA accuracy increased with the increase of components. For LDA, I tested different learning \n",
    "# rates and decay and settled on the above configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears TF-IDF is the winner!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
