{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Predicting sentiment\n",
    "In this assignment, you will be using the same sentiment analysis dataset as for Assignment 1, but you'll be looking to actually predict sentiment based on a variety of text-derived features.\n",
    "\n",
    "This dataset comes from [Mass et. al. (2011)](https://www.aclweb.org/anthology/P11-1015.pdf) and the full version is available [here](http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from collections import Counter\n",
    "import re\n",
    "from numpy import log, mean\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'pandas', 'transformers==2.4.1'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "I've saved a subset of the data in the data directory on the repository.  It is available as a pickled dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neg', 1233), ('pos', 1266)]\n",
      "[1233 1266]\n"
     ]
    }
   ],
   "source": [
    "# you will need to change this to where ever the file is stored\n",
    "data_location = './data/assignment_1_reviews.pkl'\n",
    "with open(data_location, 'rb') as f:\n",
    "    all_text = pickle.load(f)\n",
    "# corpora size\n",
    "print([(k, len(all_text[k])) for k in all_text])\n",
    "neg, pos = all_text.values()\n",
    "# for this assignment, let's combine all our data, but maintain the labels\n",
    "all_text = neg+pos\n",
    "first_pos = len(neg)\n",
    "# array makes for easier indexing\n",
    "is_positive = np.array([False]*len(neg)+[True]*len(pos))\n",
    "# check that they're equivalent\n",
    "print(np.bincount(is_positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating document feature vectors\n",
    "In this section, process all of your text data in order to create the following document-level feature vectors:\n",
    "\n",
    "- Word Counts (using `CountVectorizer`)\n",
    "- TF-IDF vectors (using `TfidfVectorizer`)\n",
    "- Non-Negative Matrix Factorization-based representations (using `NMF`)\n",
    "- Latent Dirichlet Allocation-based representations (using `LatentDirichletAllocation`)\n",
    "\n",
    "All of the design elements are up to you (e.g. tokenization, vocabulary limits, number of components).  It may make sense to try out a few different designs.  In the next section we'll do some evaluation of our different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and instantiate English model from Spacy\n",
    "from spacy.lang.en import English\n",
    "en = English()\n",
    "\n",
    "# Import and instantiaite trained model from Spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(doc, model=en):\n",
    "    '''Tokenizer based on example from from week_1_intro notebook.\n",
    "    Filters non-alpha, url-like, and stopwords then lemmatizes each parsed token.'''\n",
    "    parsed = model(doc)\n",
    "    # Return list of lowercase parsed tokens that are alphanumeric and not urls \n",
    "    return([t.lemma_ for t in parsed if (t.is_alpha) and (not t.like_url) and (not t.is_stop)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get count vectors using fit transform on a count vectorizer\n",
    "def get_count_vectors(text, cv=cv):\n",
    "    return cv.fit_transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get count dictionary using count vectors and count vectorizer\n",
    "def get_count_dict(count_vectors, cv=cv):\n",
    "    return dict(zip(cv.get_feature_names(), count_vectors.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word counts and represent as dict\n",
    "count_vectors = get_count_vectors(all_text)\n",
    "count_dict = get_count_dict(count_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate simple tfidf vectorizer\n",
    "tfidf_v = TfidfVectorizer(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_vectors(text, tfidf_v=tfidf_v):\n",
    "    return tfidf_v.fit_transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF using CountVectorizer and tokenizer\n",
    "def get_tfidf_dict(tfidf_vectors, tfidf_v=tfidf_v):\n",
    "    return dict(zip(tfidf_v.get_feature_names(), tfidf_vectors.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tfidf vectors and represent as dict - each word is the key and the tfidf number is the value\n",
    "tfidf_vectors = get_tfidf_vectors(all_text)\n",
    "all_text_tfidf_dict = get_tfidf_dict(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF using the tfidf vectors\n",
    "# Using 10 components - will explore different numbers later in the assignment\n",
    "nmf_n = 10\n",
    "nmf = NMF(n_components=nmf_n)\n",
    "nmf_vectors = nmf.fit_transform(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA using the count_vectors\n",
    "# Using 10 components - will explore different numbers later in the assignment\n",
    "lda_n = 10\n",
    "lda = LatentDirichletAllocation(n_components=lda_n)\n",
    "lda_vectors = lda.fit_transform(count_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis on vectors\n",
    "It's important to do some initial exploration of the features you've engineered.  Remember the goal is to get some information out of text, so you want to ensure your features are informative.  In this case, informative would mean it gives some information about sentiment.\n",
    "\n",
    "Perform the following analysis and any additional checks that might be useful for creating a set of informative features:\n",
    "- Top words for positive versus negative (Counts and TF-IDF)\n",
    "- Topic model performance measures (NMF=Reconstruction error, LDA=Evidence Lower BOund (ELBO))\n",
    "- Average cosine similarity between negative review vecvtors and positive review vectors (for all vectors you've created)\n",
    "\n",
    "Tip: You can use the is_positive vector to subset your vectors.  You will likely need to have them in dense array format (use the `.toarray()` method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_words(corpus, cv=cv, num_words=10):\n",
    "    '''Gets the most frequent words in a corpus, using a count vectorizer on the generated corpus dict'''\n",
    "    corpus_dict = get_corpus_dict(corpus, cv)\n",
    "    return sorted(corpus_dict, key=corpus_dict.get, reverse=True)[:num_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_dict(corpus, cv=cv):\n",
    "    '''Creates a dictionary of the words and their counts in the corpus using a count vectorizer'''\n",
    "    v = cv.fit_transform(corpus).toarray()\n",
    "    corpus_dict = dict(zip(cv.get_feature_names(), v.sum(axis=0)))\n",
    "    return corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top words by count in negative reviews are: ['movie', 'film', 'like', 'bad', 'good', 'time', 'story', 'people', 'br', 'movies']\n",
      "The top words by count in positive reviews are: ['film', 'movie', 'like', 'good', 'great', 'story', 'time', 'best', 'love', 'br']\n"
     ]
    }
   ],
   "source": [
    "# Top 10 words - counts\n",
    "# Negative reviews\n",
    "neg_words_top = get_most_frequent_words(neg)\n",
    "print(f'The top words by count in negative reviews are: {neg_words_top}')\n",
    "\n",
    "# Positive reviews\n",
    "pos_words_top = get_most_frequent_words(pos)\n",
    "print(f'The top words by count in positive reviews are: {pos_words_top}')\n",
    "\n",
    "# Note: these are not very informative! As we've seen so far in the course, word counts are not the best \n",
    "# metric for determining sentiment, as evidenced by the overlap between these two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tried using the trained nlp english mode, but there wasn't a significant improvement in performance.\n",
    "# However, theere was a significant decrease in efficiency, so I'm removing it from the tf-idf comptutation below.\n",
    "def nlp_tokenizer(doc, model=nlp):\n",
    "    '''Tokenizer based on example from from week_1_intro notebook.\n",
    "    Filters non-alpha, url-like, and stopwords then lemmatizes each parsed token.\n",
    "    Uses advanced trained nlp model from spacy by default.'''\n",
    "    parsed = model(doc)\n",
    "    # Return list of lowercase parsed tokens that are alphanumeric and not urls \n",
    "    return([t.lemma_ for t in parsed if (t.is_alpha) and (not t.like_url) and (not t.is_stop)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impose a min and max df to ensure the words aren't too frequent or infrequent\n",
    "tfidf_v2 = TfidfVectorizer(tokenizer=tokenizer, min_df=0.01, max_df=0.9)\n",
    "\n",
    "def get_top_tfidf_words(text, tfidf_v=tfidf_v2, num_words=10):\n",
    "    '''Gets the top words from tfidf vectors, based on tfidf values'''\n",
    "    tfidf_vectors = get_tfidf_vectors(text, tfidf_v=tfidf_v2)\n",
    "    tfidf_dict = dict(zip(tfidf_v.get_feature_names(), tfidf_vectors.sum(axis=0)))\n",
    "    return sorted(tfidf_dict, key=tfidf_dict.get, reverse=True)[:num_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top words by tfidf in positive reviews are: ['movie', 'film', 'good', 'like', 'great', 'story', 'time', 'best', 'love', 'watch']\n",
      "The top words by tfidf in negative reviews are: ['movie', 'film', 'like', 'bad', 'good', 'time', 'story', 'br', 'people', 'movies']\n"
     ]
    }
   ],
   "source": [
    "# Top 10 words - TF-IDF\n",
    "# Positive\n",
    "pos_top_tfidf_words = get_top_tfidf_words(pos)\n",
    "print(f'The top words by tfidf in positive reviews are: {pos_top_tfidf_words}')\n",
    "\n",
    "# Negative\n",
    "neg_top_tfidf_words = get_top_tfidf_words(neg)\n",
    "print(f'The top words by tfidf in negative reviews are: {neg_top_tfidf_words}')\n",
    "\n",
    "# Note: as you can see, these don't appear to be very helpful either! I'm thinking we will need a more \n",
    "# nuanced approach to understanding sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from week_2_vectors\n",
    "def display_components(model, word_features, top_display=5):\n",
    "    # utility for displaying respresentative words per component for topic models\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        top_words_idx = topic.argsort()[::-1][:top_display]\n",
    "        top_words = [word_features[i] for i in top_words_idx]\n",
    "        print(\" \".join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_nmf(text, n_components=10):\n",
    "    '''Helper function that initializes an nmf'''\n",
    "    # Set random state for reproducability\n",
    "    # Init with 'nndsvd' because according to the documentation it is better for sparseness\n",
    "    # Added alpha value to regularize, so calculation is more stable\n",
    "    nmf = NMF(n_components=n_components, alpha=0.1, random_state=101, init='nndsvd')\n",
    "    tfidf_vectors = get_tfidf_vectors(text, tfidf_v=tfidf_v2)\n",
    "    nmf_vectors = nmf.fit_transform(tfidf_vectors)\n",
    "    return nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reconstruction error for NMF with 30 components is: 46.08429797250744\n",
      "The reconstruction error for NMF with 50 components is: 45.027862802972145\n",
      "The reconstruction error for NMF with 75 components is: 43.891218393080685\n",
      "The reconstruction error for NMF with 100 components is: 42.85723091916044\n",
      "The reconstruction error for NMF with 150 components is: 40.95881272754923\n"
     ]
    }
   ],
   "source": [
    "# Compare reconstruction error for different number of components\n",
    "for n_components in [30,50,75,100,150]:\n",
    "    nmf = init_nmf(all_text, n_components)\n",
    "    # Top model performance metrics for nmf \n",
    "    print(f'The reconstruction error for NMF with {n_components} components is: {nmf.reconstruction_err_}')\n",
    "    \n",
    "# Note: this takes a long time to run, so I've saved the results: I tried 3,7,10,15 components, but\n",
    "# the error rate significantly decreased with 30+ components. I ultimately chose 75 because there was a slight elbow,\n",
    "# indicating that the gains by further increases were not necessarily worth the extra computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "film heard directed quality makes making director previous hollywood festival\n",
      "Topic 1:\n",
      "movie kind theater horrible main makes amazing nudity sure friend\n",
      "Topic 2:\n",
      "wife police husband murder crime woman french law tries car\n",
      "Topic 3:\n",
      "like look feel looks trying guys totally let cut makes\n",
      "Topic 4:\n",
      "br audience opera okay money line director screenplay actor credit\n",
      "Topic 5:\n",
      "series episodes television final carry excellent usual end season release\n",
      "Topic 6:\n",
      "plot twists better acting cast audience sub twist holes surprising\n",
      "Topic 7:\n",
      "book read novel books adaptation completely version changes totally parts\n",
      "Topic 8:\n",
      "bad acting badly gave guys ridiculous low flick usually pretty\n",
      "Topic 9:\n",
      "funny comedy laugh jokes comedies humor humour comedic hilarious romantic\n",
      "Topic 10:\n",
      "seen absolutely worse having times know read maybe far possible\n",
      "Topic 11:\n",
      "horror gore blood scary creepy slasher suspense scared genre flick\n",
      "Topic 12:\n",
      "great greatest acting ending brilliant job wonderful casting makes not\n",
      "Topic 13:\n",
      "movies watched comment entertaining imdb enjoy naked mind cast silly\n",
      "Topic 14:\n",
      "fun perfect humor animation enjoyable moments plenty entertaining enjoy disney\n",
      "Topic 15:\n",
      "love wonderful loved romantic falls beautiful husband girl fall happy\n",
      "Topic 16:\n",
      "action van fight sequences martial scenes arts western gun adventure\n",
      "Topic 17:\n",
      "camera shot times sound screen editing long angles shots dialog\n",
      "Topic 18:\n",
      "family true home coming wonderful brother class father town relationships\n",
      "Topic 19:\n",
      "story stories line told worth telling based short king amazing\n",
      "Topic 20:\n",
      "watch times chance sit hours got laugh watched friends entertaining\n",
      "Topic 21:\n",
      "music musical city soundtrack songs art culture band rock important\n",
      "Topic 22:\n",
      "scene scenes room left final looks appears away instead fight\n",
      "Topic 23:\n",
      "war soldier charlie german soldiers robert anti battle hollywood sub\n",
      "Topic 24:\n",
      "effects special acting budget decent scary look low better worse\n",
      "Topic 25:\n",
      "video version store available rented home day local release copy\n",
      "Topic 26:\n",
      "game games playing level play bond cool played real greatest\n",
      "Topic 27:\n",
      "old years year ago friends production remember street classic later\n",
      "Topic 28:\n",
      "time waste long got money big screen spent simply complete\n",
      "Topic 29:\n",
      "little bit found kid parts touch dead hard obvious things\n",
      "Topic 30:\n",
      "sex violence got female joe girl hot pretty filled sexy\n",
      "Topic 31:\n",
      "people real know hate kind etc reality friends kill government\n",
      "Topic 32:\n",
      "worst horrible acting script garbage redeeming writing rating poor come\n",
      "Topic 33:\n",
      "character main actress ryan development end male annoying self emotions\n",
      "Topic 34:\n",
      "films silent low recent certainly genre budget look hollywood type\n",
      "Topic 35:\n",
      "characters dialogue annoying main viewer care sequel half major portrayed\n",
      "Topic 36:\n",
      "best cast beautiful excellent perfect deserved songs award cop academy\n",
      "Topic 37:\n",
      "school high day girl students gay history student hot popular\n",
      "Topic 38:\n",
      "man mr footage end dead comes help experience river leading\n",
      "Topic 39:\n",
      "star fans rock bit wrong battle better need band david\n",
      "Topic 40:\n",
      "sci fi earth space alien planet channel budget crew b\n",
      "Topic 41:\n",
      "dr doctor son running short bit budget hospital dangerous door\n",
      "Topic 42:\n",
      "think better romance especially maybe liked watched sure mind job\n",
      "Topic 43:\n",
      "guy girl guys stupid train home hitchcock gets fight gay\n",
      "Topic 44:\n",
      "thing right fact mean god actually know absolutely ending hero\n",
      "Topic 45:\n",
      "work writers getting makes level piece earlier production money fails\n",
      "Topic 46:\n",
      "young ryan robert jack lee taylor girl early woman filmed\n",
      "Topic 47:\n",
      "actors director script performances writer production sure cast realize crazy\n",
      "Topic 48:\n",
      "want absolutely better money away sit guess know reason actually\n",
      "Topic 49:\n",
      "tv shows better days real lots got remember george right\n",
      "Topic 50:\n",
      "way better bit come right need end long different science\n",
      "Topic 51:\n",
      "good acting pretty job nice bit makes thriller easy idea\n",
      "Topic 52:\n",
      "saw thought liked seeing found corny loved expect theater idea\n",
      "Topic 53:\n",
      "terrible acting money awful stupid worse friends lines poorly understand\n",
      "Topic 54:\n",
      "lot know pretty ending big unfortunately bizarre joe screen right\n",
      "Topic 55:\n",
      "episode season episodes south hope favorite park new finally makes\n",
      "Topic 56:\n",
      "original version sequel remake hollywood island production etc classic hitchcock\n",
      "Topic 57:\n",
      "women men woman beautiful male female director big violence culture\n",
      "Topic 58:\n",
      "mother father son daughter child death wants law relationship mom\n",
      "Topic 59:\n",
      "looking forward look park viewing thinking bond poor slow maybe\n",
      "Topic 60:\n",
      "world true open attitude human set americans dark new art\n",
      "Topic 61:\n",
      "fan recommend big highly cartoon bit huge definitely long laughs\n",
      "Topic 62:\n",
      "black white rock band jack pick humor named read water\n",
      "Topic 63:\n",
      "dvd buy released watched live version today ago sound students\n",
      "Topic 64:\n",
      "role performance plays actor played play cast lead supporting job\n",
      "Topic 65:\n",
      "boring interesting minutes end gets manage clich√© point ended pointless\n",
      "Topic 66:\n",
      "monster creature giant footage reason goes stars awful god jack\n",
      "Topic 67:\n",
      "cinema history silent city piece directors humour hope human indian\n",
      "Topic 68:\n",
      "japanese american americans culture end jack left hollywood interest soldiers\n",
      "Topic 69:\n",
      "kids children parents child adults sad message magic remember let\n",
      "Topic 70:\n",
      "life real gives brought convincing lives death job woman documentary\n",
      "Topic 71:\n",
      "find gets group works lost indian actually mean probably couple\n",
      "Topic 72:\n",
      "watching worth enjoy house etc not sat eye friend art\n",
      "Topic 73:\n",
      "julie rock excellent highly singing group brown dark michael trying\n",
      "Topic 74:\n",
      "going island end poor girl boat hell completely minutes things\n"
     ]
    }
   ],
   "source": [
    "nmf_n = 75\n",
    "nmf = NMF(n_components=nmf_n, random_state=101)\n",
    "tfidf_vectors_all = get_tfidf_vectors(all_text, tfidf_v=tfidf_v2)\n",
    "nmf_vectors_all = nmf.fit_transform(tfidf_vectors_all)\n",
    "\n",
    "# Print out the components and analyze them\n",
    "display_components(nmf, tfidf_v2.get_feature_names(), 10)\n",
    "\n",
    "# Note: for 10 components, they appear to be split along genres and/or types of movies/TV.\n",
    "# This split might be helpful, but it's still unclear exactly how this might be useful. Again, ultimately I landed on\n",
    "# 75 as the number of components, which is what I've included above. 75 components also appears to split along\n",
    "# similar lines, but also includes production type (e.g. low-quality, camera styles, etc.). Much more informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new count vectorizer with min and max df\n",
    "cv2 = CountVectorizer(tokenizer=tokenizer, min_df=0.01, max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lda(text, n_components):\n",
    "    '''Helper function to initialize an lDA'''\n",
    "    # Increase the learning offset slightly to favor later reviews in the learning process\n",
    "    # I played aroun with learning decay, but didn't find anything better than the default rate of 0.7\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, learning_offset=15, random_state=101)\n",
    "    count_vectors = cv2.fit_transform(text).toarray()\n",
    "    lda_vectors = lda.fit_transform(count_vectors)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bound for LDA with 1 components is: 893.6286335855411\n",
      "The bound for LDA with 2 components is: 885.4677455027298\n",
      "The bound for LDA with 3 components is: 901.1347060471696\n",
      "The bound for LDA with 4 components is: 911.5578782461239\n",
      "The bound for LDA with 5 components is: 926.3874138501176\n",
      "The bound for LDA with 6 components is: 938.8797631098181\n",
      "The bound for LDA with 7 components is: 953.125259602031\n",
      "The bound for LDA with 8 components is: 968.8322869166778\n"
     ]
    }
   ],
   "source": [
    "# Compare lda bounds for different number of components\n",
    "for n_components in range(1,9):\n",
    "    lda = init_lda(all_text, n_components)\n",
    "    # Top model performance metrics for nmf \n",
    "    print(f'The bound for LDA with {n_components} components is: {lda.bound_}')\n",
    "    \n",
    "# Note: it appears that the bound value increases with the number of components. I tried 30, 50, 100, 150, 300, \n",
    "# and higher and the values kept getting worse. So I tried 1-15 and then realized I could use 1-8 to \n",
    "# tune hyperparameters and found that 2 was the best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "film story like br good films great life time man love character best characters way young world scenes director little\n",
      "Topic 1:\n",
      "movie like film good bad time people watch movies think acting know seen plot great watching way better story funny\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "lda_n = 2\n",
    "lda_all = LatentDirichletAllocation(n_components=lda_n, random_state=101)\n",
    "count_vectors_all = cv2.fit_transform(all_text).toarray()\n",
    "lda_vectors_all = lda_all.fit_transform(count_vectors_all)\n",
    "\n",
    "# Print out the components and analyze them\n",
    "display_components(lda_all, cv2.get_feature_names(), 20)\n",
    "\n",
    "# Note: while 2 components might have the lowest bound, it doesn't give much information when displayed with the\n",
    "# above function. \n",
    "# I also analyzed 10 components and 20 components. The split was more helpful in this case, but still didn't seem\n",
    "# very useful for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(COUNT) neg-to-neg: 0.12424753323103478 neg-to-pos: 0.11098543653407493 pos-to-pos: 0.11192883224959577\n",
      "(TF-IDF) neg-to-neg: 0.056494811837617544 neg-to-pos: 0.048939370583884215 pos-to-pos: 0.05228368790762318\n",
      "(NMF) neg-to-neg: 0.19259946727305174 neg-to-pos: 0.17267149168558535 pos-to-pos: 0.18170457858479086\n",
      "(LDA) neg-to-neg: 0.7974850336027157 neg-to-pos: 0.6980764700131532 pos-to-pos: 0.7741226389173657\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity - counts\n",
    "count_sims = cosine_similarity(count_vectors_all)\n",
    "\n",
    "# Cosine similarity - tfidf\n",
    "tfidf_sims = cosine_similarity(tfidf_vectors_all)\n",
    "\n",
    "# Cosine similarity - nmf\n",
    "nmf_sims = cosine_similarity(nmf_vectors_all)\n",
    "\n",
    "# Cosine similarity - lda\n",
    "lda_sims = cosine_similarity(lda_vectors_all)\n",
    "\n",
    "sims_data = {'(COUNT)': count_sims, '(TF-IDF)': tfidf_sims, '(NMF)': nmf_sims, '(LDA)': lda_sims}\n",
    "\n",
    "# compare positive to negative average distance\n",
    "# code from week_2_vectors\n",
    "for key, s_matrix in sims_data.items():\n",
    "    print(f'{key} neg-to-neg:', s_matrix[:first_pos, :first_pos].mean(axis=1).mean(),\n",
    "          'neg-to-pos:', s_matrix[:first_pos, first_pos:].mean(axis=1).mean(),\n",
    "          'pos-to-pos:', s_matrix[first_pos:, first_pos:].mean(axis=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the above results look? Ideally you should see that your features give some information that might help a model discern negative from positive reviews.  That means lower similarity inter-class and different words showing up as most frequent/relevant.  Experiment with your design choices on the steps above.  Your goal should be to get to a set of vectors that have lower inter-class similarity than intra-class similarity (e.g. positive reviews should be more similar to positive reviews than negative reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generally speaking for all of the comparison types, intra-class similarity is higher than inter-class, \n",
    "# but not by much for pos-to-pos. This is especially true for the count comparison. It seems like negatives are \n",
    "# more similar to each other than positives or positive-negatives. This might be explained by the type of review - \n",
    "# people are more likely to be emphatic and passionate about their negative reviews than their positive reviews. It\n",
    "# might be the case that this sentiment became encoded in the analysis. \n",
    "#Overall, itappears to be successful collection of calculations. \n",
    "\n",
    "# I included my notes above on various approaches I explored to optimize the different vector outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting sentiment\n",
    "As we did in week 2's notebook, we're now going to use these informative vectors to predict sentiment.  We'll be using `LinearSVC` in this exercise, but feel free to try out other models.\n",
    "\n",
    "Start by creating a train/test split for the dataset (typically 70%/30%).  We'll use the same split for all feature vectors for comparability. \n",
    "\n",
    "Do the following steps for all the feature vectors you developed above:\n",
    "- Start by creating a train/test split for the dataset (typically 70%/30%).  We'll use the same split for all feature vectors for comparability. \n",
    "- Train an SVM model on your feature vectors with the corresponding target values (positive/negative)\n",
    "- Test the SVM model on the test set and output the accuracy\n",
    "\n",
    "Tip: Sklearn has a train/test split functionality for generating train/test splits (`sklearn.model_selection.train_test_split`).  Since we want to use the same reviews, make sure you set a random_state (see the docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0]*len(neg)+[1]*len(pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_text, y, test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how you've designed your vectors, you may find that the topic models perform worse than the count vectors.  You may want to try a couple different configurations.  \n",
    "\n",
    "One key reason for this may be because if the goal is to use our test observations to simulate our \"new observations\", we haven't properly done that.  We've fit our vectorizers on the FULL corpus.  If our test observations are \"unseen\", that means our vectorizers should only be fit on the training corpus.\n",
    "\n",
    "Try this out: Split the unprocessed reviews, fit the vectorizer, then the model and then transform the test observations and predict.  See how the accuracy changes\n",
    "\n",
    "Tip: You may want to explore sklearn's `Pipelines`, which is designed for exactly this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer_accuracy(vectorizer, X_train, X_test, y_train, y_test):\n",
    "    # Initialize linearsvc model\n",
    "    model = LinearSVC(random_state=101)\n",
    "    # Get vectors of training data\n",
    "    train_vectors = vectorizer.fit_transform(X_train).toarray()\n",
    "    # Fit the model to that data\n",
    "    model.fit(train_vectors, y_train)\n",
    "    # Get the test vectors for the data\n",
    "    test_vectors = vectorizer.transform(X_test).toarray()\n",
    "    # Get the test predictions\n",
    "    test_preds = model.predict(test_vectors)\n",
    "    return accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for count is: 0.8133333333333334\n"
     ]
    }
   ],
   "source": [
    "svc_count = LinearSVC(random_state=101)\n",
    "count_accuracy = get_vectorizer_accuracy(cv, X_train, X_test, y_train, y_test)\n",
    "print(f'The accuracy for count is: {count_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for TF-IDF is: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Instantiate simple tfidf vectorizer\n",
    "tfidf_v3 = TfidfVectorizer(tokenizer=tokenizer)\n",
    "tfidf_accuracy = get_vectorizer_accuracy(tfidf_v3, X_train, X_test, y_train, y_test)\n",
    "print(f'The accuracy for TF-IDF is: {tfidf_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_v4 = TfidfVectorizer(tokenizer=tokenizer)\n",
    "\n",
    "# Note: this function and the lda one can be combined with get_vectorizer_accuracy, I just ran out of time...\n",
    "\n",
    "def get_nmf_accuracy(X_train, X_test, y_train, y_test):\n",
    "    # Initialize svc model\n",
    "    model = LinearSVC(random_state=101)\n",
    "    # Initialize nmf\n",
    "    nmf = NMF(n_components=7, random_state=101)\n",
    "    # Get tfidf vectors for train data\n",
    "    tfidf_train_vectors = tfidf_v4.fit_transform(X_train).toarray()\n",
    "    # Use nmf to fit transform the tfidf vectors for train data\n",
    "    train_vectors = nmf.fit_transform(tfidf_train_vectors)\n",
    "    # Fit the svc model\n",
    "    model.fit(train_vectors, y_train)\n",
    "    # Use nmf to fit transform the tfidf vectors for test data\n",
    "    tfidf_test_vectors = tfidf_v4.fit_transform(X_test).toarray()\n",
    "    # Get the test vectors\n",
    "    test_vectors = nmf.fit_transform(tfidf_test_vectors)\n",
    "    # Make test predictions\n",
    "    test_preds = model.predict(test_vectors)\n",
    "    return accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for NMF is: 0.5293333333333333\n"
     ]
    }
   ],
   "source": [
    "nmf_accuracy = get_nmf_accuracy(X_train, X_test, y_train, y_test)\n",
    "print(f'The accuracy for NMF is: {nmf_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv4 = CountVectorizer(tokenizer=tokenizer)\n",
    "\n",
    "def get_lda_accuracy(X_train, X_test, y_train, y_test):\n",
    "    # Initialize svc model\n",
    "    model = LinearSVC(random_state=101)\n",
    "    # Initialize lda\n",
    "    lda = LatentDirichletAllocation(n_components=98, learning_offset=15, random_state=101)\n",
    "    # Get count vectors for train data\n",
    "    count_train_vectors = cv4.fit_transform(X_train).toarray()\n",
    "    # Use nmf to fit transform the count vectors for train data\n",
    "    train_vectors = lda.fit_transform(count_train_vectors)\n",
    "    # Fit the svc model\n",
    "    model.fit(train_vectors, y_train)\n",
    "    # Use nmf to fit transform the tfidf vectors for test data\n",
    "    count_test_vectors = cv4.fit_transform(X_test).toarray()\n",
    "    # Get the test vectors\n",
    "    test_vectors = lda.fit_transform(count_test_vectors)\n",
    "    # Make test predictions\n",
    "    test_preds = model.predict(test_vectors)\n",
    "    return accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for LDA is: 0.516\n"
     ]
    }
   ],
   "source": [
    "lda_accuracy = get_lda_accuracy(X_train, X_test, y_train, y_test)\n",
    "print(f'The accuracy for LDA is: {lda_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I found that updating the number of components for nmf and lda had significant impact on the accuracy.\n",
    "# But it was the inverse of what I found in previous sections: for NMF, accuracy decreased with an increase of \n",
    "# components; for LDA accuracy increased with the increase of components. For LDA, I tested different learning \n",
    "# rates and decay and settled on the above configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears TF-IDF is the winner!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
