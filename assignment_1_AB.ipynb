{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Tokenization and Word counts for sentiment analysis\n",
    "In this assignment, you will be applying the techniques learned in week 1 of the course to perform and analyze sentiment on a dataset of movie reviews from IMDB.\n",
    "\n",
    "This dataset comes from [Mass et. al. (2011)](https://www.aclweb.org/anthology/P11-1015.pdf) and the full version is available [here](http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from collections import Counter\n",
    "import re\n",
    "from numpy import log, mean\n",
    "\n",
    "required = {'spacy', 'scikit-learn', 'pandas', 'transformers==2.4.1'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "en = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "\n",
    "I've already processed the full dataset for you and saved it as a data file: `assignment_1_reviews.pkl`.  You don't need to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neg', 1233), ('pos', 1266)]\n"
     ]
    }
   ],
   "source": [
    "# you will need to change this to where ever the file is stored\n",
    "# on colab, you can likely just put this as 'assignment_1_reviews.pkl'\n",
    "data_location = './data/assignment_1_reviews.pkl'\n",
    "with open(data_location, 'rb') as f:\n",
    "    all_text = pickle.load(f)\n",
    "# corpora size\n",
    "print([(k, len(all_text[k])) for k in all_text])\n",
    "# for simplicity, let's split these into separate sets\n",
    "neg, pos = all_text.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Use what you've developed in the week 1 notebook to tokenize each of the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenizer based on example from from week_1_intro notebook\n",
    "def simple_tokenizer(doc, model=en):\n",
    "    parsed = model(doc)\n",
    "    # Return list of lowercase parsed tokens that are alphanumeric and not urls \n",
    "    return([t.lower_ for t in parsed if (t.is_alpha) and (not t.like_url)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the negative and positive reviews\n",
    "neg_tokenized = [simple_tokenizer(doc) for doc in neg]\n",
    "pos_tokenized = [simple_tokenizer(doc) for doc in pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counts\n",
    "Create a count of the number of words in each review.  Use scikit-learn's CountVectorizer.  Refer to the documentation as it has a few parameters you might want to think about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple count vectorizer user the simple tokenizer\n",
    "simple_vectorizer = CountVectorizer(tokenizer=simple_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fit-transformed vectors of the negative reviews\n",
    "neg_vectors = simple_vectorizer.fit_transform(neg).toarray()\n",
    "neg_count_dict = dict(zip(simple_vectorizer.get_feature_names(), neg_vectors.sum(axis=0)))\n",
    "print(neg_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vectors = simple_vectorizer.fit_transform(pos).toarray()\n",
    "pos_count_dict = dict(zip(simple_vectorizer.get_feature_names(), pos_vectors.sum(axis=0)))\n",
    "print(pos_count_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent words\n",
    "What are the top 10 most frequent words in the positive reviews? The negative reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_dict(corpus, cv):\n",
    "    v = cv.fit_transform(corpus).toarray()\n",
    "    corpus_dict = dict(zip(cv.get_feature_names(), v.sum(axis=0)))\n",
    "    return corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_words(corpus, cv=simple_vectorizer, num_words=10):\n",
    "    '''Convenience function using the logic above'''\n",
    "    corpus_dict = get_corpus_dict(corpus, cv)\n",
    "    return sorted(corpus_dict, key=corpus_dict.get, reverse=True)[:num_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words_top = get_most_frequent_words(neg)\n",
    "print(neg_words_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words_top = get_most_frequent_words(pos)\n",
    "print(pos_words_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there's a lot of pretty irrelevant words in the top here.  It's hard to really say anything about this.  Can you think of a way to get to more informative terms (i.e. ones that might give you some insight as to what words are positive versus negative?)\n",
    "\n",
    "Hint: Think about which tokens might be less informative.  Is there a way we learned to remove those?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a trained model to improve lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_tokenizer(doc, model=nlp):\n",
    "    \"Advanced tokenizer based on example from from week_1_intro notebook\"\n",
    "    parsed = model(doc)\n",
    "    # Return list of lowercase parsed tokens that are alphanumeric and not urls \n",
    "    return([t.lemma_ for t in parsed if (t.is_alpha) and (not t.like_url) and (not t.is_stop)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize count vectorizer with advanced parameters\n",
    "# Added stop words to tokenizer rather than vectorizer so that stop words list is consistent with model used\n",
    "advanced_vectorizer = CountVectorizer(tokenizer=advanced_tokenizer, min_df=0.1, max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words_top_advanced = get_most_frequent_words(neg, cv=advanced_vectorizer, num_words=30)\n",
    "print(neg_words_top_advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words_top_advanced = get_most_frequent_words(pos, cv=advanced_vectorizer, num_words=30)\n",
    "print(pos_words_top_advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how often the top words from negative appear in the positive reviews and vice versa.  Do these seem like good candidates for determining whether a review is positive or negative? If not, maybe expand to the top 10, or more.  The idea here is to get a list of terms that are pretty distinct between the two sets.\n",
    "\n",
    "One possible way to test is to use [log-likelihood ratio](https://wordhoard.northwestern.edu/userman/analysis-comparewords.html) as we discussed in class. In class we looked at texts with/without mentions of \"hot dog\".  What is our comparison text in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log, mean\n",
    "# Code from course notebook, adjusted to deal with words rather than dataframes and to return g rather than print it\n",
    "def log_likelihood(analysis, reference, word):\n",
    "    # count of word in source\n",
    "    a = analysis[word]\n",
    "    # count of word in reference\n",
    "    b = reference[word]\n",
    "    # count of all words in source\n",
    "    c = len(analysis)\n",
    "    # count of all words in reference\n",
    "    d = len(reference)\n",
    "    e1 = c*(a+b)/(c+d)\n",
    "    e2 = d*(a+b)/(c+d)\n",
    "    g = 2*((a*log(a/e1)) + (b*log(b/e2)))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_corpus_dict = get_corpus_dict(pos, advanced_vectorizer)\n",
    "neg_corpus_dict = get_corpus_dict(neg, advanced_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_likelihood_list(analysis_dict, reference_dict, words):\n",
    "    log_dict = {}\n",
    "    for word in words:\n",
    "        g = log_likelihood(analysis_dict, reference_dict, word)\n",
    "        log_dict[word] = g\n",
    "    return sorted(log_dict, key=log_dict.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_log_list = get_log_likelihood_list(pos_corpus_dict, neg_corpus_dict, pos_words_top_advanced)\n",
    "print(pos_log_dict_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_log_list = get_log_likelihood_list(neg_corpus_dict, pos_corpus_dict, neg_words_top_advanced)\n",
    "print(neg_log_dict_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary-based sentiment analysis \n",
    "Construct a list of the keywords you've found are good determinants if a review is positive or negative.  Use this list to \"score\" a review based on the number of times that word appears in the review.\n",
    "\n",
    "(Optional) A quick and fancy way of doing this is to use CountVectorizer's vocabulary parameter.  Think how you might be able to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos_review in pos:\n",
    "    for word in pos_review:\n",
    "        if word in pos_log_dict_sorted:\n",
    "            print('great')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did you do? How often do the negative reviews have a higher negative score than a positive score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based sentiment analysis\n",
    "Above we did some tinkering with our scoring and found it works to some extent, but it's likely not going to work the same on another dataset.  That is, it's not particularly generalizable.  However, modern sentiment analysis has moved away from dictionary-based scoring towards having sentiment be a \"classification\" problem.  \n",
    "\n",
    "For this last section, take a look at the transformers [Pipelines](https://github.com/huggingface/transformers#quick-tour-of-pipelines) functionality.  You'll see that with a few lines of code you can bring in an advanced sentiment analysis model.  Run this against the positive/negative corpus and see how it works compared to your work above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
